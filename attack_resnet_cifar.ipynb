{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import isfile\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet34\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from helper_functions import train_or_load, create_eval_post_loader, create_post_train_loader, evaluate_attack_model, DatasetClassN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Datasets\n",
    "SHADOW_DATA_PATH = \"pickle/cifar10/resnet34/shadow.p\"\n",
    "EVALUATE_DATA_PATH = \"pickle/cifar10/resnet34/eval.p\"\n",
    "# Save Dset create for attack model training\n",
    "ATT_TRAIN_DATA_PATH = \"pickle/cifar10/resnet34/attack_train.p\"\n",
    "## Models\n",
    "SHADOW_MODEL_PATH = \"shadow_models/resnet34_shadow_cifar_overtrained.pth\"\n",
    "TARGET_MODEL_PATH = \"models/resnet34_cifar10.pth\"\n",
    "ATTACK_MODEL_PATH = \"attack_models/attack_resnet_cifar.pth\"\n",
    "## Attack model training parameters\n",
    "LEARNING_R = 0.0001\n",
    "EPOCHS = 5\n",
    "MULTI_WORKERS_N = 2\n",
    "\n",
    "DEVICE=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shadow Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shadow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the shadow model trained in the other python script\n",
    "resnet_shadow = resnet34(weights=None,num_classes=DatasetClassN.Cifar.value).to(DEVICE) #resnet_target is the shadow model\n",
    "resnet_shadow.load_state_dict(torch.load(SHADOW_MODEL_PATH, map_location=DEVICE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shadow dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SHADOW_DATA_PATH, \"rb\") as f:\n",
    "    dataset = pickle.load(f)\n",
    "    \n",
    "train_data, val_data = train_test_split(dataset, test_size=(1-0.5),shuffle=False)\n",
    "  \n",
    "trainloader = DataLoader(train_data, batch_size=1, shuffle=False, num_workers=MULTI_WORKERS_N)\n",
    "testloader =  DataLoader(val_data, batch_size=1, shuffle=True, num_workers=MULTI_WORKERS_N)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attack Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Attack model training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_train_loader = create_post_train_loader(testloader, trainloader, resnet_shadow, batch_size=64, \\\n",
    "    multi_n= MULTI_WORKERS_N, device=DEVICE, save_path=ATT_TRAIN_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attack Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttackNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AttackNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 32)\n",
    "        self.bn1 = nn.BatchNorm1d(32)  # Matches the output of fc1\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)  # Matches the output of fc2\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.bn3 = nn.BatchNorm1d(32)  # Matches the output of fc3\n",
    "        self.fc4 = nn.Linear(32, 16)\n",
    "        self.bn4 = nn.BatchNorm1d(16)  # Matches the output of fc4\n",
    "        self.fc5 = nn.Linear(16, 8)\n",
    "        self.bn5 = nn.BatchNorm1d(8)   # Matches the output of fc5\n",
    "        self.fc6 = nn.Linear(8, 1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.fc1(x)))\n",
    "        x = torch.relu(self.bn2(self.fc2(x)))\n",
    "        x = torch.relu(self.bn3(self.fc3(x)))\n",
    "        x = torch.relu(self.bn4(self.fc4(x)))\n",
    "        x = self.dropout(torch.relu(self.bn5(self.fc5(x))))\n",
    "        x = self.sigmoid(self.fc6(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Attack model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "tensor([[8.2618, 0.9000, 0.6849]])\n",
      "tensor([1.])\n",
      "Batch 0 with Loss: 0.9331651329994202\n",
      "tensor([[12.1451, -0.6677, -4.9435]])\n",
      "tensor([1.])\n",
      "Batch 50 with Loss: 0.8166717290878296\n",
      "tensor([[16.4935,  1.0791, -2.1723]])\n",
      "tensor([0.])\n",
      "Batch 100 with Loss: 0.6835198402404785\n",
      "tensor([[10.6089,  1.8982, -0.6269]])\n",
      "tensor([1.])\n",
      "Batch 150 with Loss: 0.7200303077697754\n",
      "tensor([[10.8100,  1.0248,  0.4482]])\n",
      "tensor([0.])\n",
      "Batch 200 with Loss: 0.7252645492553711\n",
      "tensor([[11.3257,  3.8776, -0.8593]])\n",
      "tensor([1.])\n",
      "Batch 250 with Loss: 0.7747058868408203\n",
      "tensor([[ 7.6587,  4.8065, -0.6778]])\n",
      "tensor([1.])\n",
      "Batch 300 with Loss: 0.7950496673583984\n",
      "tensor([[10.3120,  3.9053,  0.2060]])\n",
      "tensor([1.])\n",
      "Batch 350 with Loss: 0.6938363909721375\n",
      "tensor([[12.0007,  0.7907, -0.0312]])\n",
      "tensor([0.])\n",
      "Batch 400 with Loss: 0.6996859312057495\n",
      "tensor([[15.1302,  0.4700, -1.5154]])\n",
      "tensor([1.])\n",
      "Batch 450 with Loss: 0.6219495534896851\n",
      "Epoch: 2\n",
      "tensor([[ 4.8903,  3.0597, -0.0720]])\n",
      "tensor([0.])\n",
      "Batch 0 with Loss: 0.7215373516082764\n",
      "tensor([[5.7444, 2.3954, 1.9021]])\n",
      "tensor([0.])\n",
      "Batch 50 with Loss: 0.7385815382003784\n",
      "tensor([[ 6.0070,  1.7804, -0.1672]])\n",
      "tensor([0.])\n",
      "Batch 100 with Loss: 0.744658350944519\n",
      "tensor([[7.0945, 1.2929, 0.0883]])\n",
      "tensor([0.])\n",
      "Batch 150 with Loss: 0.6692468523979187\n",
      "tensor([[2.5132, 1.8839, 0.2977]])\n",
      "tensor([0.])\n",
      "Batch 200 with Loss: 0.7389681339263916\n",
      "tensor([[10.8428,  5.2248, -0.4256]])\n",
      "tensor([0.])\n",
      "Batch 250 with Loss: 0.6488379240036011\n",
      "tensor([[20.7522,  0.7205, -3.2096]])\n",
      "tensor([0.])\n",
      "Batch 300 with Loss: 0.7644493579864502\n",
      "tensor([[6.5106, 4.7833, 0.1275]])\n",
      "tensor([1.])\n",
      "Batch 350 with Loss: 0.7371768355369568\n",
      "tensor([[7.0654, 3.0212, 1.6415]])\n",
      "tensor([1.])\n",
      "Batch 400 with Loss: 0.6453171372413635\n",
      "tensor([[11.5882, -1.0832, -1.2215]])\n",
      "tensor([1.])\n",
      "Batch 450 with Loss: 0.7275336980819702\n",
      "Epoch: 3\n",
      "tensor([[ 8.8873,  3.9100, -0.1219]])\n",
      "tensor([1.])\n",
      "Batch 0 with Loss: 0.6780303716659546\n",
      "tensor([[3.2243, 1.6592, 0.8624]])\n",
      "tensor([1.])\n",
      "Batch 50 with Loss: 0.7093506455421448\n",
      "tensor([[ 8.7356, -0.3438, -1.2961]])\n",
      "tensor([0.])\n",
      "Batch 100 with Loss: 0.7248533368110657\n",
      "tensor([[11.7676,  4.6831,  1.7774]])\n",
      "tensor([0.])\n",
      "Batch 150 with Loss: 0.7298151850700378\n",
      "tensor([[3.3552, 2.3912, 1.8549]])\n",
      "tensor([0.])\n",
      "Batch 200 with Loss: 0.6454280018806458\n",
      "tensor([[5.8000, 1.6786, 1.2780]])\n",
      "tensor([0.])\n",
      "Batch 250 with Loss: 0.6927278637886047\n",
      "tensor([[1.9009, 0.7810, 0.4796]])\n",
      "tensor([0.])\n",
      "Batch 300 with Loss: 0.7084287405014038\n",
      "tensor([[4.7304, 3.1110, 0.8545]])\n",
      "tensor([0.])\n",
      "Batch 350 with Loss: 0.6914419531822205\n",
      "tensor([[3.3920, 2.5969, 1.9064]])\n",
      "tensor([0.])\n",
      "Batch 400 with Loss: 0.6720567941665649\n",
      "tensor([[8.4698, 1.1361, 0.7972]])\n",
      "tensor([0.])\n",
      "Batch 450 with Loss: 0.7311719655990601\n",
      "Epoch: 4\n",
      "tensor([[7.1974, 0.4108, 0.0218]])\n",
      "tensor([1.])\n",
      "Batch 0 with Loss: 0.6364909410476685\n",
      "tensor([[ 5.5027,  2.7765, -0.3810]])\n",
      "tensor([0.])\n",
      "Batch 50 with Loss: 0.703659176826477\n",
      "tensor([[ 8.3296,  2.6012, -0.8944]])\n",
      "tensor([1.])\n",
      "Batch 100 with Loss: 0.6478182077407837\n",
      "tensor([[7.4346, 2.5541, 1.3158]])\n",
      "tensor([1.])\n",
      "Batch 150 with Loss: 0.6572217345237732\n",
      "tensor([[11.9836,  0.1949, -0.3144]])\n",
      "tensor([0.])\n",
      "Batch 200 with Loss: 0.6948215365409851\n",
      "tensor([[5.0541, 1.1115, 0.4829]])\n",
      "tensor([1.])\n",
      "Batch 250 with Loss: 0.7024072408676147\n",
      "tensor([[5.3390, 2.4496, 1.2489]])\n",
      "tensor([1.])\n",
      "Batch 300 with Loss: 0.6841919422149658\n",
      "tensor([[ 9.6139,  1.7868, -0.0941]])\n",
      "tensor([1.])\n",
      "Batch 350 with Loss: 0.6488532423973083\n",
      "tensor([[13.8730,  1.4284, -0.7019]])\n",
      "tensor([1.])\n",
      "Batch 400 with Loss: 0.6874461770057678\n",
      "tensor([[ 8.5992,  1.6193, -1.2530]])\n",
      "tensor([1.])\n",
      "Batch 450 with Loss: 0.666388750076294\n",
      "Epoch: 5\n",
      "tensor([[3.9383, 2.9234, 2.5864]])\n",
      "tensor([0.])\n",
      "Batch 0 with Loss: 0.6954712867736816\n",
      "tensor([[ 3.2051,  2.3107, -0.5265]])\n",
      "tensor([0.])\n",
      "Batch 50 with Loss: 0.6583873629570007\n",
      "tensor([[ 8.7819,  1.0703, -0.9580]])\n",
      "tensor([0.])\n",
      "Batch 100 with Loss: 0.6619834303855896\n",
      "tensor([[4.1544, 1.1844, 1.0470]])\n",
      "tensor([0.])\n",
      "Batch 150 with Loss: 0.6756927371025085\n",
      "tensor([[5.9496, 2.6065, 1.6546]])\n",
      "tensor([0.])\n",
      "Batch 200 with Loss: 0.6835684776306152\n",
      "tensor([[2.7640, 1.4832, 1.3619]])\n",
      "tensor([0.])\n",
      "Batch 250 with Loss: 0.6486215591430664\n",
      "tensor([[ 7.2108,  2.2401, -0.1864]])\n",
      "tensor([0.])\n",
      "Batch 300 with Loss: 0.6763229370117188\n",
      "tensor([[ 7.4239, -0.3730, -0.6941]])\n",
      "tensor([0.])\n",
      "Batch 350 with Loss: 0.7108905911445618\n",
      "tensor([[8.1626, 1.6330, 0.8449]])\n",
      "tensor([1.])\n",
      "Batch 400 with Loss: 0.6678574681282043\n",
      "tensor([[13.3344,  1.8554, -1.8767]])\n",
      "tensor([1.])\n",
      "Batch 450 with Loss: 0.6459243297576904\n"
     ]
    }
   ],
   "source": [
    "attack_model = AttackNN()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(attack_model.parameters(), lr=LEARNING_R)\n",
    "\n",
    "train_or_load(attack_model, attack_train_loader, optimizer, criterion, EPOCHS, ATTACK_MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EVALUATE_DATA_PATH, \"rb\") as eval_f:\n",
    "    eval_dataset = pickle.load(eval_f)\n",
    "    \n",
    "eval_dl = DataLoader(eval_dataset, batch_size=1 , shuffle=False, num_workers=MULTI_WORKERS_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Target Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load target model\n",
    "resnet_target_model = resnet34(weights=None,num_classes=DatasetClassN.Cifar.value)\n",
    "resnet_target_model.load_state_dict (torch.load (TARGET_MODEL_PATH, map_location=DEVICE)[\"net\"])\n",
    "resnet_target_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posteriors + Member Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "create_eval_post_loader() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m eval_dl_post \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_eval_post_loader\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mresnet_target_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMULTI_WORKERS_N\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: create_eval_post_loader() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "eval_dl_post = create_eval_post_loader (resnet_target_model, eval_dl, MULTI_WORKERS_N, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_attack_model(attack_model, optimizer, eval_dl_post)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
