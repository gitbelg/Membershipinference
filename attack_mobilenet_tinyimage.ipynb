{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "#import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "from helper_functions import train_or_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User-1\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User-1\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the shadow model trained in the other python script\n",
    "device = device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet_shadow =  models.mobilenet_v2(weights=None,num_classes = 200)\n",
    "\n",
    "resnet_cifar = torch.load(\"shadow_models/mobilenet_shadow_tinyimage_overtrained.pth\",map_location=device)\n",
    "resnet_shadow.load_state_dict(resnet_cifar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'pickle/tinyimagenet/mobilenetv2/shadow.p'\n",
    "# Change the DATA_PATH to your local pickle file path\n",
    "\n",
    "\n",
    "with open(DATA_PATH, \"rb\") as f:\n",
    "    dataset = pickle.load(f)\n",
    "\n",
    "\n",
    "#splitting\n",
    "#only use train set here\n",
    "train_data, val_data = train_test_split(dataset, test_size=(1-0.5),shuffle=False)\n",
    "  \n",
    "dataloader = torch.utils.data.DataLoader(train_data, batch_size=1, shuffle=False, num_workers=2)\n",
    "testloader =  torch.utils.data.DataLoader(val_data, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "for batch_idx, (img, label) in enumerate(dataloader):\n",
    "    img = img.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate dataset for attack model training\n",
    "resnet_shadow.eval()\n",
    "dataset_attack = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in testloader: #need only one\n",
    "            # Move images and labels to the appropriate device\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "        logits = resnet_shadow(images)\n",
    "        \n",
    "        #take the 3 biggest logist\n",
    "        top_values = torch.topk(logits, k=3).values\n",
    "        top_values, indices = torch.sort(top_values, dim=1, descending=True)\n",
    "        dataset_attack.append([top_values,0])\n",
    "        \n",
    "with torch.no_grad():\n",
    "    for images, labels in dataloader: #need only one\n",
    "            # Move images and labels to the appropriate device\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "        logits = resnet_shadow(images)\n",
    "        \n",
    "        #take the 3 biggest logist\n",
    "        \n",
    "        top_values = torch.topk(logits, k=3).values\n",
    "        top_values, indices = torch.sort(top_values, dim=1, descending=True)\n",
    "\n",
    "        dataset_attack.append([top_values,1])\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_data_list = dataset_attack\n",
    "\n",
    "# # Convert all tensors to the same dtype first\n",
    "# tensors = [data[0].float() for data in dataset_attack]  # Ensure all tensors are Float type\n",
    "# all_data = torch.cat(tensors, dim=0)  # Concatenate all tensors\n",
    "\n",
    "# # Calculate mean and std\n",
    "# mean = all_data.mean(dim=0)\n",
    "# std = all_data.std(dim=0)\n",
    "\n",
    "# # Standardize data in the list\n",
    "# standardized_data_list = [( (data[0] - mean) / std, data[1] ) for data in dataset_attack]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_attack = torch.utils.data.DataLoader(standardized_data_list, batch_size=64, shuffle=True, num_workers=2) #shuffled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 32)\n",
    "        self.bn1 = nn.BatchNorm1d(32)  # Matches the output of fc1\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)  # Matches the output of fc2\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.bn3 = nn.BatchNorm1d(32)  # Matches the output of fc3\n",
    "        self.fc4 = nn.Linear(32, 16)\n",
    "        self.bn4 = nn.BatchNorm1d(16)  # Matches the output of fc4\n",
    "        self.fc5 = nn.Linear(16, 8)\n",
    "        self.bn5 = nn.BatchNorm1d(8)   # Matches the output of fc5\n",
    "        self.fc6 = nn.Linear(8, 1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.fc1(x)))\n",
    "        x = torch.relu(self.bn2(self.fc2(x)))\n",
    "        x = torch.relu(self.bn3(self.fc3(x)))\n",
    "        x = torch.relu(self.bn4(self.fc4(x)))\n",
    "        x = self.dropout(torch.relu(self.bn5(self.fc5(x))))\n",
    "        x = self.sigmoid(self.fc6(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_model = SimpleNN()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(attack_model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict =  torch.load('attack_model.pth_3', map_location='cpu')\n",
    "#model.load_state_dict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "tensor([[1.7859, 1.1660, 1.0927]])\n",
      "tensor([0.])\n",
      "Batch 0 with Loss: 0.7742007970809937\n",
      "tensor([[2.2882, 0.9182, 0.8732]])\n",
      "tensor([1.])\n",
      "Batch 50 with Loss: 0.7192357182502747\n",
      "tensor([[3.7081, 2.1735, 1.9821]])\n",
      "tensor([1.])\n",
      "Batch 100 with Loss: 0.7025209665298462\n",
      "tensor([[3.3852, 2.0995, 2.0257]])\n",
      "tensor([0.])\n",
      "Batch 150 with Loss: 0.7233453989028931\n",
      "tensor([[2.7052, 2.0890, 1.5056]])\n",
      "tensor([0.])\n",
      "Batch 200 with Loss: 0.6828556656837463\n",
      "tensor([[0.9985, 0.3835, 0.3269]])\n",
      "tensor([1.])\n",
      "Batch 250 with Loss: 0.7053641080856323\n",
      "tensor([[0.5660, 0.4249, 0.3271]])\n",
      "tensor([0.])\n",
      "Batch 300 with Loss: 0.6885164976119995\n",
      "tensor([[2.1141, 1.7693, 1.6562]])\n",
      "tensor([1.])\n",
      "Batch 350 with Loss: 0.7109758853912354\n",
      "tensor([[1.5047, 1.0248, 1.0113]])\n",
      "tensor([1.])\n",
      "Batch 400 with Loss: 0.7347774505615234\n",
      "tensor([[0.9441, 0.6927, 0.5244]])\n",
      "tensor([1.])\n",
      "Batch 450 with Loss: 0.708190381526947\n",
      "tensor([[1.6871, 1.6283, 1.5531]])\n",
      "tensor([0.])\n",
      "Batch 500 with Loss: 0.7166533470153809\n",
      "tensor([[1.8333, 1.6350, 1.5659]])\n",
      "tensor([0.])\n",
      "Batch 550 with Loss: 0.7339149713516235\n",
      "tensor([[1.3598, 0.7045, 0.5829]])\n",
      "tensor([0.])\n",
      "Batch 600 with Loss: 0.7309478521347046\n",
      "tensor([[1.1124, 1.0618, 0.8458]])\n",
      "tensor([1.])\n",
      "Batch 650 with Loss: 0.7281190752983093\n",
      "tensor([[1.7597, 1.1597, 1.1386]])\n",
      "tensor([1.])\n",
      "Batch 700 with Loss: 0.7075327634811401\n",
      "tensor([[2.5029, 2.1935, 1.5805]])\n",
      "tensor([0.])\n",
      "Batch 750 with Loss: 0.7156652212142944\n",
      "Epoch: 2\n",
      "tensor([[5.9414, 4.0756, 3.7403]])\n",
      "tensor([1.])\n",
      "Batch 0 with Loss: 0.71007239818573\n",
      "tensor([[2.8306, 2.3314, 1.3399]])\n",
      "tensor([0.])\n",
      "Batch 50 with Loss: 0.7423077821731567\n",
      "tensor([[4.9474, 3.8151, 3.2349]])\n",
      "tensor([1.])\n",
      "Batch 100 with Loss: 0.7012894153594971\n",
      "tensor([[3.2119, 1.8249, 1.7874]])\n",
      "tensor([1.])\n",
      "Batch 150 with Loss: 0.7082681059837341\n",
      "tensor([[1.3367, 1.1011, 1.0070]])\n",
      "tensor([1.])\n",
      "Batch 200 with Loss: 0.7238712906837463\n",
      "tensor([[1.9404, 0.9174, 0.6783]])\n",
      "tensor([1.])\n",
      "Batch 250 with Loss: 0.715320885181427\n",
      "tensor([[2.5655, 2.4865, 2.2763]])\n",
      "tensor([1.])\n",
      "Batch 300 with Loss: 0.6986619830131531\n",
      "tensor([[2.1702, 1.8258, 1.3026]])\n",
      "tensor([0.])\n",
      "Batch 350 with Loss: 0.7215734720230103\n",
      "tensor([[1.4533, 0.9739, 0.7812]])\n",
      "tensor([0.])\n",
      "Batch 400 with Loss: 0.6803903579711914\n",
      "tensor([[1.2942, 0.7642, 0.5734]])\n",
      "tensor([1.])\n",
      "Batch 450 with Loss: 0.7023782134056091\n",
      "tensor([[4.0710, 3.7968, 2.3000]])\n",
      "tensor([1.])\n",
      "Batch 500 with Loss: 0.7079589366912842\n",
      "tensor([[0.4014, 0.3976, 0.2388]])\n",
      "tensor([1.])\n",
      "Batch 550 with Loss: 0.6770650744438171\n",
      "tensor([[2.6507, 1.5335, 1.3750]])\n",
      "tensor([0.])\n",
      "Batch 600 with Loss: 0.6869893670082092\n",
      "tensor([[1.0116, 0.7198, 0.5667]])\n",
      "tensor([0.])\n",
      "Batch 650 with Loss: 0.679348349571228\n",
      "tensor([[1.0744, 0.8771, 0.7469]])\n",
      "tensor([0.])\n",
      "Batch 700 with Loss: 0.7380606532096863\n",
      "tensor([[1.7211, 1.2478, 0.9557]])\n",
      "tensor([1.])\n",
      "Batch 750 with Loss: 0.6773849725723267\n",
      "Epoch: 3\n",
      "tensor([[2.1838, 0.9439, 0.7092]])\n",
      "tensor([0.])\n",
      "Batch 0 with Loss: 0.6887150406837463\n",
      "tensor([[2.3198, 1.8585, 1.2794]])\n",
      "tensor([1.])\n",
      "Batch 50 with Loss: 0.6993755102157593\n",
      "tensor([[1.5410, 1.0929, 0.6971]])\n",
      "tensor([1.])\n",
      "Batch 100 with Loss: 0.7259819507598877\n",
      "tensor([[1.5909, 1.3955, 1.3762]])\n",
      "tensor([0.])\n",
      "Batch 150 with Loss: 0.6910272836685181\n",
      "tensor([[1.4050, 0.5119, 0.4978]])\n",
      "tensor([1.])\n",
      "Batch 200 with Loss: 0.6991703510284424\n",
      "tensor([[1.0076, 0.5957, 0.5249]])\n",
      "tensor([0.])\n",
      "Batch 250 with Loss: 0.684528112411499\n",
      "tensor([[ 0.1995, -0.0398, -0.1782]])\n",
      "tensor([1.])\n",
      "Batch 300 with Loss: 0.7032109498977661\n",
      "tensor([[4.2399, 3.7125, 2.5200]])\n",
      "tensor([1.])\n",
      "Batch 350 with Loss: 0.7059297561645508\n",
      "tensor([[ 0.7800,  0.3100, -0.1229]])\n",
      "tensor([0.])\n",
      "Batch 400 with Loss: 0.6903547644615173\n",
      "tensor([[1.0655, 0.7353, 0.6843]])\n",
      "tensor([0.])\n",
      "Batch 450 with Loss: 0.688980221748352\n",
      "tensor([[2.4007, 1.8975, 1.8323]])\n",
      "tensor([1.])\n",
      "Batch 500 with Loss: 0.6913150548934937\n",
      "tensor([[1.6588, 1.3029, 1.2734]])\n",
      "tensor([0.])\n",
      "Batch 550 with Loss: 0.6954232454299927\n",
      "tensor([[3.8336, 2.6688, 1.7736]])\n",
      "tensor([0.])\n",
      "Batch 600 with Loss: 0.7046143412590027\n",
      "tensor([[0.9254, 0.3169, 0.3003]])\n",
      "tensor([1.])\n",
      "Batch 650 with Loss: 0.6969813704490662\n",
      "tensor([[1.0353, 0.5279, 0.3130]])\n",
      "tensor([0.])\n",
      "Batch 700 with Loss: 0.6813279986381531\n",
      "tensor([[2.5205, 2.0220, 1.9113]])\n",
      "tensor([1.])\n",
      "Batch 750 with Loss: 0.6953111886978149\n",
      "Epoch: 4\n",
      "tensor([[2.6880, 1.9491, 1.7171]])\n",
      "tensor([0.])\n",
      "Batch 0 with Loss: 0.6999077796936035\n",
      "tensor([[4.5404, 3.5852, 3.3378]])\n",
      "tensor([1.])\n",
      "Batch 50 with Loss: 0.691254198551178\n",
      "tensor([[1.5363, 1.4516, 1.3228]])\n",
      "tensor([1.])\n",
      "Batch 100 with Loss: 0.677040159702301\n",
      "tensor([[2.2620, 1.0020, 0.1840]])\n",
      "tensor([1.])\n",
      "Batch 150 with Loss: 0.7020145654678345\n",
      "tensor([[1.9054, 1.3794, 0.9388]])\n",
      "tensor([0.])\n",
      "Batch 200 with Loss: 0.7156492471694946\n",
      "tensor([[2.3775, 1.9791, 1.8320]])\n",
      "tensor([0.])\n",
      "Batch 250 with Loss: 0.6725180149078369\n",
      "tensor([[1.9166, 1.4451, 1.1733]])\n",
      "tensor([1.])\n",
      "Batch 300 with Loss: 0.7154756784439087\n",
      "tensor([[1.9320, 1.6653, 1.3082]])\n",
      "tensor([0.])\n",
      "Batch 350 with Loss: 0.6912341117858887\n",
      "tensor([[0.7661, 0.5743, 0.5343]])\n",
      "tensor([0.])\n",
      "Batch 400 with Loss: 0.679826021194458\n",
      "tensor([[1.9584, 1.8872, 1.6108]])\n",
      "tensor([1.])\n",
      "Batch 450 with Loss: 0.6979877352714539\n",
      "tensor([[1.1501, 1.1294, 1.0997]])\n",
      "tensor([1.])\n",
      "Batch 500 with Loss: 0.7020726203918457\n",
      "tensor([[1.8018, 1.6003, 0.8726]])\n",
      "tensor([1.])\n",
      "Batch 550 with Loss: 0.701326847076416\n",
      "tensor([[4.4522, 3.6039, 3.4719]])\n",
      "tensor([0.])\n",
      "Batch 600 with Loss: 0.6790656447410583\n",
      "tensor([[2.6158, 1.7378, 1.0617]])\n",
      "tensor([0.])\n",
      "Batch 650 with Loss: 0.6969065070152283\n",
      "tensor([[0.2943, 0.1448, 0.0351]])\n",
      "tensor([1.])\n",
      "Batch 700 with Loss: 0.6907222867012024\n",
      "tensor([[1.4086, 1.3110, 1.2678]])\n",
      "tensor([1.])\n",
      "Batch 750 with Loss: 0.6927210092544556\n",
      "Epoch: 5\n",
      "tensor([[1.0127, 0.7722, 0.7565]])\n",
      "tensor([0.])\n",
      "Batch 0 with Loss: 0.7071343064308167\n",
      "tensor([[2.9807, 2.0289, 1.7231]])\n",
      "tensor([0.])\n",
      "Batch 50 with Loss: 0.7170829772949219\n",
      "tensor([[0.9871, 0.9152, 0.7136]])\n",
      "tensor([1.])\n",
      "Batch 100 with Loss: 0.6980671286582947\n",
      "tensor([[3.6099, 3.5517, 2.1138]])\n",
      "tensor([1.])\n",
      "Batch 150 with Loss: 0.7040167450904846\n",
      "tensor([[2.0637, 1.8759, 1.8524]])\n",
      "tensor([0.])\n",
      "Batch 200 with Loss: 0.6983603239059448\n",
      "tensor([[1.3013, 0.2939, 0.1516]])\n",
      "tensor([1.])\n",
      "Batch 250 with Loss: 0.6841616630554199\n",
      "tensor([[0.4002, 0.2878, 0.0342]])\n",
      "tensor([1.])\n",
      "Batch 300 with Loss: 0.7134723663330078\n",
      "tensor([[1.5492, 1.0475, 0.9211]])\n",
      "tensor([0.])\n",
      "Batch 350 with Loss: 0.6906153559684753\n",
      "tensor([[4.0235, 3.9723, 3.3821]])\n",
      "tensor([1.])\n",
      "Batch 400 with Loss: 0.6911008358001709\n",
      "tensor([[ 1.8144,  0.1207, -0.0041]])\n",
      "tensor([1.])\n",
      "Batch 450 with Loss: 0.6833042502403259\n",
      "tensor([[2.3622, 1.7978, 1.4699]])\n",
      "tensor([0.])\n",
      "Batch 500 with Loss: 0.6901876330375671\n",
      "tensor([[2.4514, 2.1669, 2.0751]])\n",
      "tensor([1.])\n",
      "Batch 550 with Loss: 0.6982517242431641\n",
      "tensor([[1.5817, 0.8454, 0.7418]])\n",
      "tensor([1.])\n",
      "Batch 600 with Loss: 0.701612651348114\n",
      "tensor([[1.8157, 1.1617, 0.7234]])\n",
      "tensor([1.])\n",
      "Batch 650 with Loss: 0.7022023797035217\n",
      "tensor([[1.4627, 0.7672, 0.5538]])\n",
      "tensor([0.])\n",
      "Batch 700 with Loss: 0.682659387588501\n",
      "tensor([[1.0905, 0.4758, 0.4744]])\n",
      "tensor([0.])\n",
      "Batch 750 with Loss: 0.6868494749069214\n"
     ]
    }
   ],
   "source": [
    "train_or_load(attack_model,dataloader_attack,optimizer,criterion,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(attack_model.state_dict(), 'attack_models/attack_mobilenet_tinyimage.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Evaluation</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"pickle/tinyimagenet/mobilenetv2/eval.p\"\n",
    "\n",
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "with open(DATA_PATH, \"rb\") as f:\n",
    "    dataset = pickle.load(f)\n",
    "# Convert all tensors to the same dtype first\n",
    "\n",
    "eval_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=1 , shuffle=False, num_workers=2)\n",
    "# #splitting\n",
    "# for batch_idx, (img, label, membership) in enumerate(dataloader):\n",
    "#     img = img.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MobileNetV2(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (14): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (15): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (16): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (17): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (18): Conv2dNormActivation(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): Linear(in_features=1280, out_features=200, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load target model\n",
    "resnet_target_model =  models.mobilenet_v2(weights=None,num_classes = 200)\n",
    "check = torch.load(\"models/mobilenetv2_tinyimagenet.pth\", map_location=device)\n",
    "resnet_target_model.load_state_dict(check[\"net\"])\n",
    "resnet_target_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get posteriors from target\n",
    "target_dataset_eval = []\n",
    "with torch.no_grad():\n",
    "    for images,_, member in eval_dataloader: #need only one\n",
    "            # Move images and labels to the appropriate device\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "        logits = resnet_target_model(images)\n",
    "        \n",
    "        #take the 3 biggest logist\n",
    "        \n",
    "        top_values = torch.topk(logits, k=3).values #order poseri\n",
    "        sorted_tensor, indices = torch.sort(top_values, dim=1,descending=True)\n",
    "        target_dataset_eval.append([sorted_tensor, member.item()])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #onvert all tensors to the same dtype first\n",
    "# tensors = [data[0].float() for data in dataset_eval]  # Ensure all tensors are Float type\n",
    "# all_data = torch.cat(tensors, dim=0)  # Concatenate all tensors\n",
    "\n",
    "# # Calculate mean and std\n",
    "# mean = all_data.mean(dim=0)\n",
    "# std = all_data.std(dim=0)\n",
    "\n",
    "# # Standardize data in the list\n",
    "# dataset = [( (data[0] - mean) / std, data[1] ) for data in dataset_eval]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_eval = torch.utils.data.DataLoader(target_dataset_eval, batch_size=1 , shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User-1\\AppData\\Local\\Temp\\ipykernel_17384\\3255356309.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[0.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[0.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[0.]]) tensor([0.])\n",
      "tensor([[0.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[0.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[0.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[0.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[0.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[0.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[0.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[0.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[0.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "Accuracy: 0.56\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for data in dataloader:\n",
    "            inputs = data[0]\n",
    "            \n",
    "            labels = data[1].float()\n",
    "            labels = labels.float()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs.squeeze(dim=1))\n",
    "            predicted = torch.round(outputs)  # Round the outputs to 0 or 1\n",
    "            print(predicted,labels)\n",
    "\n",
    "            total += labels.size(0)  # Increment the total count by batch size\n",
    "            correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "accuracy = evaluate(attack_model, dataloader_eval)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
