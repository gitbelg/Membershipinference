{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "#import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the shadow model trained in the other python script\n",
    "device = \"cpu\"\n",
    "resnet_shadow =  models.mobilenet_v2(pretrained=False)\n",
    "num_classes = 10  # Change this to the number of classes in your dataset\n",
    "num_features = resnet_shadow.classifier[1].in_features\n",
    "resnet_shadow.classifier = nn.Sequential(\n",
    "    nn.Linear(num_features, num_classes)\n",
    ")\n",
    "\n",
    "resnet_cifar = torch.load(\"mobilenet_shadow_cifar_overtrained.pth\",map_location=torch.device('cpu'))\n",
    "\n",
    "resnet_shadow.load_state_dict(resnet_cifar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'shadow.p'\n",
    "# Change the DATA_PATH to your local pickle file path\n",
    "\n",
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "with open(DATA_PATH, \"rb\") as f:\n",
    "    dataset = pickle.load(f)\n",
    "\n",
    "\n",
    "#splitting\n",
    "#only use train set here\n",
    "train_data, val_data = train_test_split(dataset, test_size=(1-0.5),shuffle=False)\n",
    "  \n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    train_data, batch_size=\n",
    "     1 , shuffle=False, num_workers=2)\n",
    "testloader =  torch.utils.data.DataLoader(val_data, batch_size=1,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "for batch_idx, (img, label) in enumerate(dataloader):\n",
    "    img = img.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate dataset for attack model\n",
    "resnet_shadow.eval()\n",
    "dataset_attack = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in testloader: #need only one\n",
    "            # Move images and labels to the appropriate device\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "        logits = resnet_shadow(images)\n",
    "        \n",
    "        #take the 3 biggest logist\n",
    "        \n",
    "        top_values = torch.topk(logits, k=3).values\n",
    "        top_values, indices = torch.sort(top_values, dim=1, descending=True)\n",
    "        dataset_attack.append([top_values,0])\n",
    "        \n",
    "with torch.no_grad():\n",
    "    for images, labels in dataloader: #need only one\n",
    "            # Move images and labels to the appropriate device\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "        logits = resnet_shadow(images)\n",
    "        \n",
    "        #take the 3 biggest logist\n",
    "        \n",
    "        top_values = torch.topk(logits, k=3).values\n",
    "        top_values, indices = torch.sort(top_values, dim=1, descending=True)\n",
    "\n",
    "        dataset_attack.append([top_values,1])\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Convert all tensors to the same dtype first\\ntensors = [data[0].float() for data in dataset_attack]  # Ensure all tensors are Float type\\nall_data = torch.cat(tensors, dim=0)  # Concatenate all tensors\\n\\n# Calculate mean and std\\nmean = all_data.mean(dim=0)\\nstd = all_data.std(dim=0)\\n\\n# Standardize data in the list\\nstandardized_data_list = [( (data[0] - mean) / std, data[1] ) for data in dataset_attack]\\n'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standardized_data_list = dataset_attack\n",
    "\"\"\"\n",
    "# Convert all tensors to the same dtype first\n",
    "tensors = [data[0].float() for data in dataset_attack]  # Ensure all tensors are Float type\n",
    "all_data = torch.cat(tensors, dim=0)  # Concatenate all tensors\n",
    "\n",
    "# Calculate mean and std\n",
    "mean = all_data.mean(dim=0)\n",
    "std = all_data.std(dim=0)\n",
    "\n",
    "# Standardize data in the list\n",
    "standardized_data_list = [( (data[0] - mean) / std, data[1] ) for data in dataset_attack]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_attack = torch.utils.data.DataLoader(\n",
    "    \n",
    "    standardized_data_list, batch_size=64, shuffle=True, num_workers=2) #shuffled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 32)\n",
    "        self.bn1 = nn.BatchNorm1d(32)  # Matches the output of fc1\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)  # Matches the output of fc2\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.bn3 = nn.BatchNorm1d(32)  # Matches the output of fc3\n",
    "        self.fc4 = nn.Linear(32, 16)\n",
    "        self.bn4 = nn.BatchNorm1d(16)  # Matches the output of fc4\n",
    "        self.fc5 = nn.Linear(16, 8)\n",
    "        self.bn5 = nn.BatchNorm1d(8)   # Matches the output of fc5\n",
    "        self.fc6 = nn.Linear(8, 1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.fc1(x)))\n",
    "        x = torch.relu(self.bn2(self.fc2(x)))\n",
    "        x = torch.relu(self.bn3(self.fc3(x)))\n",
    "        x = torch.relu(self.bn4(self.fc4(x)))\n",
    "        x = self.dropout(torch.relu(self.bn5(self.fc5(x))))\n",
    "        x = self.sigmoid(self.fc6(x))\n",
    "        return x\n",
    "\n",
    "# Ensure input dimension is correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNN()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your training function\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion, epochs):\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.float()  # Ensures input tensors are floats\n",
    "        labels = labels.float().view(-1, 1)  # Ensures labels are floats and reshaped correctly\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.squeeze(dim = 1))\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNN(\n",
       "  (fc1): Linear(in_features=3, out_features=32, bias=True)\n",
       "  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc2): Linear(in_features=32, out_features=64, bias=True)\n",
       "  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc3): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc4): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (bn4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc5): Linear(in_features=16, out_features=8, bias=True)\n",
       "  (bn5): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc6): Linear(in_features=8, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict =  torch.load('attack_model.pth_3', map_location='cpu')\n",
    "#model.load_state_dict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for inputs in dataloader_attack:\\n    inputs = inputs[0]\\n    print(inputs.shape)  # Check input shape consistenc\\n    print(inputs)\\n    print(inputs)\\n    outputs = model(inputs.squeeze(dim = 1))'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"for inputs in dataloader_attack:\n",
    "    inputs = inputs[0]\n",
    "    print(inputs.shape)  # Check input shape consistenc\n",
    "    print(inputs)\n",
    "    print(inputs)\n",
    "    outputs = model(inputs.squeeze(dim = 1))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6902701258659363\n",
      "Loss: 0.6743139624595642\n",
      "Loss: 0.6922743916511536\n",
      "Loss: 0.6667091846466064\n",
      "Loss: 0.6869983673095703\n",
      "Loss: 0.7223072648048401\n",
      "Loss: 0.6834421753883362\n",
      "Loss: 0.6674128770828247\n",
      "Loss: 0.6828210353851318\n",
      "Loss: 0.7042727470397949\n",
      "Loss: 0.6898802518844604\n",
      "Loss: 0.7122758030891418\n",
      "Loss: 0.7144584059715271\n",
      "Loss: 0.6870189905166626\n",
      "Loss: 0.6901620626449585\n",
      "Loss: 0.6994820833206177\n",
      "Loss: 0.701591968536377\n",
      "Loss: 0.6752540469169617\n",
      "Loss: 0.680755078792572\n",
      "Loss: 0.666269838809967\n",
      "Loss: 0.7113248109817505\n",
      "Loss: 0.6863154172897339\n",
      "Loss: 0.6828051805496216\n",
      "Loss: 0.7177075743675232\n",
      "Loss: 0.6980864405632019\n",
      "Loss: 0.6898539662361145\n",
      "Loss: 0.7343880534172058\n",
      "Loss: 0.6785268783569336\n",
      "Loss: 0.6828323602676392\n",
      "Loss: 0.6908277869224548\n",
      "Loss: 0.7017245292663574\n",
      "Loss: 0.6839150190353394\n",
      "Loss: 0.6825065612792969\n",
      "Loss: 0.713539719581604\n",
      "Loss: 0.681577205657959\n",
      "Loss: 0.6963977217674255\n",
      "Loss: 0.7059194445610046\n",
      "Loss: 0.6956728100776672\n",
      "Loss: 0.6964032053947449\n",
      "Loss: 0.7000255584716797\n",
      "Loss: 0.6912937760353088\n",
      "Loss: 0.6832588911056519\n",
      "Loss: 0.7054828405380249\n",
      "Loss: 0.6737490892410278\n",
      "Loss: 0.7039395570755005\n",
      "Loss: 0.6533159017562866\n",
      "Loss: 0.688498854637146\n",
      "Loss: 0.6791070699691772\n",
      "Loss: 0.6828882694244385\n",
      "Loss: 0.6817825436592102\n",
      "Loss: 0.6709263324737549\n",
      "Loss: 0.687041699886322\n",
      "Loss: 0.6859216690063477\n",
      "Loss: 0.7100901007652283\n",
      "Loss: 0.6852418780326843\n",
      "Loss: 0.6694955825805664\n",
      "Loss: 0.6913608908653259\n",
      "Loss: 0.6982685923576355\n",
      "Loss: 0.7046133279800415\n",
      "Loss: 0.6954058408737183\n",
      "Loss: 0.700301468372345\n",
      "Loss: 0.6746441721916199\n",
      "Loss: 0.6947542428970337\n",
      "Loss: 0.6602444648742676\n",
      "Loss: 0.6897596716880798\n",
      "Loss: 0.7002719640731812\n",
      "Loss: 0.671278178691864\n",
      "Loss: 0.6812975406646729\n",
      "Loss: 0.698100745677948\n",
      "Loss: 0.6969261169433594\n",
      "Loss: 0.7062214016914368\n",
      "Loss: 0.6920164227485657\n",
      "Loss: 0.6663671731948853\n",
      "Loss: 0.7010363340377808\n",
      "Loss: 0.690682053565979\n",
      "Loss: 0.680477499961853\n",
      "Loss: 0.6951668858528137\n",
      "Loss: 0.689958930015564\n",
      "Loss: 0.693567156791687\n",
      "Loss: 0.6726560592651367\n",
      "Loss: 0.7029257416725159\n",
      "Loss: 0.6752710938453674\n",
      "Loss: 0.7110425233840942\n",
      "Loss: 0.6742544770240784\n",
      "Loss: 0.6786627173423767\n",
      "Loss: 0.7038984298706055\n",
      "Loss: 0.7043989300727844\n",
      "Loss: 0.6831667423248291\n",
      "Loss: 0.6685639023780823\n",
      "Loss: 0.6836497783660889\n",
      "Loss: 0.6917774081230164\n",
      "Loss: 0.6887531876564026\n",
      "Loss: 0.6715819835662842\n",
      "Loss: 0.6854472160339355\n",
      "Loss: 0.7151809930801392\n",
      "Loss: 0.6831705570220947\n",
      "Loss: 0.6853649020195007\n",
      "Loss: 0.6673204898834229\n",
      "Loss: 0.6952120661735535\n",
      "Loss: 0.6816368103027344\n",
      "Loss: 0.6803600788116455\n",
      "Loss: 0.6888809204101562\n",
      "Loss: 0.6749249696731567\n",
      "Loss: 0.6904594302177429\n",
      "Loss: 0.6864618062973022\n",
      "Loss: 0.6919617652893066\n",
      "Loss: 0.658987283706665\n",
      "Loss: 0.6913059949874878\n",
      "Loss: 0.7098127603530884\n",
      "Loss: 0.6998341679573059\n",
      "Loss: 0.6767870187759399\n",
      "Loss: 0.7096014022827148\n",
      "Loss: 0.6802467703819275\n",
      "Loss: 0.6901485919952393\n",
      "Loss: 0.6965113878250122\n",
      "Loss: 0.6835280060768127\n",
      "Loss: 0.6887097954750061\n",
      "Loss: 0.6878347992897034\n",
      "Loss: 0.7221342325210571\n",
      "Loss: 0.6741316914558411\n",
      "Loss: 0.6872111558914185\n",
      "Loss: 0.7044272422790527\n",
      "Loss: 0.6967669129371643\n",
      "Loss: 0.6989054679870605\n",
      "Loss: 0.6855593323707581\n",
      "Loss: 0.7099295854568481\n",
      "Loss: 0.6811994314193726\n",
      "Loss: 0.6791857481002808\n",
      "Loss: 0.6628374457359314\n",
      "Loss: 0.690641462802887\n",
      "Loss: 0.6986180543899536\n",
      "Loss: 0.6926184892654419\n",
      "Loss: 0.7101510763168335\n",
      "Loss: 0.6798735857009888\n",
      "Loss: 0.67111736536026\n",
      "Loss: 0.7041834592819214\n",
      "Loss: 0.6938654184341431\n",
      "Loss: 0.7006468176841736\n",
      "Loss: 0.6685358881950378\n",
      "Loss: 0.6965824365615845\n",
      "Loss: 0.6671847701072693\n",
      "Loss: 0.6873875260353088\n",
      "Loss: 0.6628559231758118\n",
      "Loss: 0.6757957935333252\n",
      "Loss: 0.6719794273376465\n",
      "Loss: 0.6930307745933533\n",
      "Loss: 0.6759132742881775\n",
      "Loss: 0.700520396232605\n",
      "Loss: 0.6903241872787476\n",
      "Loss: 0.6861951351165771\n",
      "Loss: 0.674213707447052\n",
      "Loss: 0.6920408010482788\n",
      "Loss: 0.6711340546607971\n",
      "Loss: 0.6838660836219788\n",
      "Loss: 0.6852810382843018\n",
      "Loss: 0.6820228099822998\n",
      "Loss: 0.6877319812774658\n",
      "Loss: 0.6841927170753479\n",
      "Loss: 0.6833442449569702\n",
      "Loss: 0.7022079229354858\n",
      "Loss: 0.6939019560813904\n",
      "Loss: 0.7062641382217407\n",
      "Loss: 0.6892760396003723\n",
      "Loss: 0.666814923286438\n",
      "Loss: 0.6872230172157288\n",
      "Loss: 0.6883273124694824\n",
      "Loss: 0.6845095753669739\n",
      "Loss: 0.6805714964866638\n",
      "Loss: 0.7132207155227661\n",
      "Loss: 0.6921095252037048\n",
      "Loss: 0.6590372323989868\n",
      "Loss: 0.7139049768447876\n",
      "Loss: 0.6822782158851624\n",
      "Loss: 0.6971814036369324\n",
      "Loss: 0.6942636966705322\n",
      "Loss: 0.6774874329566956\n",
      "Loss: 0.6797048449516296\n",
      "Loss: 0.6828837394714355\n",
      "Loss: 0.6809349656105042\n",
      "Loss: 0.7040309309959412\n",
      "Loss: 0.6918959021568298\n",
      "Loss: 0.6886691451072693\n",
      "Loss: 0.6864945292472839\n",
      "Loss: 0.6842210292816162\n",
      "Loss: 0.6868933439254761\n",
      "Loss: 0.6737053990364075\n",
      "Loss: 0.6788678169250488\n",
      "Loss: 0.684893786907196\n",
      "Loss: 0.6706477403640747\n",
      "Loss: 0.6910963654518127\n",
      "Loss: 0.7193949222564697\n",
      "Loss: 0.6883551478385925\n",
      "Loss: 0.6715098023414612\n",
      "Loss: 0.6870298385620117\n",
      "Loss: 0.6871244311332703\n",
      "Loss: 0.696877658367157\n",
      "Loss: 0.7057593464851379\n",
      "Loss: 0.697837233543396\n",
      "Loss: 0.6814584732055664\n",
      "Loss: 0.6858876943588257\n",
      "Loss: 0.668318510055542\n",
      "Loss: 0.6786676645278931\n",
      "Loss: 0.7034595012664795\n",
      "Loss: 0.686403751373291\n",
      "Loss: 0.6989220380783081\n",
      "Loss: 0.6981244683265686\n",
      "Loss: 0.688605010509491\n",
      "Loss: 0.7149165868759155\n",
      "Loss: 0.6917819380760193\n",
      "Loss: 0.6814863681793213\n",
      "Loss: 0.6771135926246643\n",
      "Loss: 0.6993721127510071\n",
      "Loss: 0.6884167790412903\n",
      "Loss: 0.7055924534797668\n",
      "Loss: 0.6966138482093811\n",
      "Loss: 0.6733452081680298\n",
      "Loss: 0.6816729307174683\n",
      "Loss: 0.7026715278625488\n",
      "Loss: 0.6871413588523865\n",
      "Loss: 0.6797264218330383\n",
      "Loss: 0.6685118675231934\n",
      "Loss: 0.701350748538971\n",
      "Loss: 0.6954325437545776\n",
      "Loss: 0.6828559637069702\n",
      "Loss: 0.6710382103919983\n",
      "Loss: 0.6802442073822021\n",
      "Loss: 0.6833345890045166\n",
      "Loss: 0.684154748916626\n",
      "Loss: 0.6795793771743774\n",
      "Loss: 0.6908719539642334\n",
      "Loss: 0.6926679611206055\n",
      "Loss: 0.6769856214523315\n",
      "Loss: 0.6727243661880493\n",
      "Loss: 0.7102400064468384\n",
      "Loss: 0.6830205917358398\n",
      "Loss: 0.6781511306762695\n",
      "Loss: 0.6877996921539307\n",
      "Loss: 0.7054874897003174\n",
      "Loss: 0.6980050206184387\n",
      "Loss: 0.7115963101387024\n",
      "Loss: 0.7103745937347412\n",
      "Loss: 0.6942680478096008\n",
      "Loss: 0.6616162061691284\n",
      "Loss: 0.6864328980445862\n",
      "Loss: 0.6861721277236938\n",
      "Loss: 0.6973371505737305\n",
      "Loss: 0.7009068727493286\n",
      "Loss: 0.6902065873146057\n",
      "Loss: 0.7220488786697388\n",
      "Loss: 0.704699695110321\n",
      "Loss: 0.6785873174667358\n",
      "Loss: 0.68913733959198\n",
      "Loss: 0.7097373604774475\n",
      "Loss: 0.6918666362762451\n",
      "Loss: 0.6627607941627502\n",
      "Loss: 0.6978078484535217\n",
      "Loss: 0.6777427196502686\n",
      "Loss: 0.6893160343170166\n",
      "Loss: 0.6873742938041687\n",
      "Loss: 0.6789079904556274\n",
      "Loss: 0.6716417670249939\n",
      "Loss: 0.7016499042510986\n",
      "Loss: 0.6831457614898682\n",
      "Loss: 0.67319655418396\n",
      "Loss: 0.6905192136764526\n",
      "Loss: 0.6989351511001587\n",
      "Loss: 0.6792829036712646\n",
      "Loss: 0.6733975410461426\n",
      "Loss: 0.6762977838516235\n",
      "Loss: 0.6781047582626343\n",
      "Loss: 0.6925745010375977\n",
      "Loss: 0.6798965334892273\n",
      "Loss: 0.6918492913246155\n",
      "Loss: 0.6734857559204102\n",
      "Loss: 0.6781963109970093\n",
      "Loss: 0.6937275528907776\n",
      "Loss: 0.6882343292236328\n",
      "Loss: 0.6759017109870911\n",
      "Loss: 0.68659508228302\n",
      "Loss: 0.695411741733551\n",
      "Loss: 0.7164272665977478\n",
      "Loss: 0.6997575163841248\n",
      "Loss: 0.684404194355011\n",
      "Loss: 0.7079254984855652\n",
      "Loss: 0.6965613961219788\n",
      "Loss: 0.6851793527603149\n",
      "Loss: 0.7033720016479492\n",
      "Loss: 0.6936588883399963\n",
      "Loss: 0.7174103856086731\n",
      "Loss: 0.700271725654602\n",
      "Loss: 0.6714272499084473\n",
      "Loss: 0.6829521656036377\n",
      "Loss: 0.6741944551467896\n",
      "Loss: 0.6918956637382507\n",
      "Loss: 0.6808913946151733\n",
      "Loss: 0.7094511389732361\n",
      "Loss: 0.6921066641807556\n",
      "Loss: 0.6813725233078003\n",
      "Loss: 0.6626988649368286\n",
      "Loss: 0.6988862752914429\n",
      "Loss: 0.700023889541626\n",
      "Loss: 0.6889122128486633\n",
      "Loss: 0.7011308670043945\n",
      "Loss: 0.6850076913833618\n",
      "Loss: 0.6762716770172119\n",
      "Loss: 0.6955949664115906\n",
      "Loss: 0.7000009417533875\n",
      "Loss: 0.6842122077941895\n",
      "Loss: 0.6769266128540039\n",
      "Loss: 0.6991570591926575\n",
      "Loss: 0.6722191572189331\n",
      "Loss: 0.668846845626831\n",
      "Loss: 0.6783307790756226\n",
      "Loss: 0.6961261630058289\n",
      "Loss: 0.6747103333473206\n",
      "Loss: 0.6577929258346558\n",
      "Loss: 0.6947333812713623\n",
      "Loss: 0.6964572668075562\n",
      "Loss: 0.6820740103721619\n",
      "Loss: 0.7027440667152405\n",
      "Loss: 0.7143228054046631\n",
      "Loss: 0.7021785974502563\n",
      "Loss: 0.6958506107330322\n",
      "Loss: 0.6921183466911316\n",
      "Loss: 0.7029793858528137\n",
      "Loss: 0.6710469722747803\n",
      "Loss: 0.6609902381896973\n",
      "Loss: 0.6952328681945801\n",
      "Loss: 0.7069798707962036\n",
      "Loss: 0.6809772849082947\n",
      "Loss: 0.6846572160720825\n",
      "Loss: 0.7002991437911987\n",
      "Loss: 0.6796181201934814\n",
      "Loss: 0.7072533369064331\n",
      "Loss: 0.685485303401947\n",
      "Loss: 0.6876638531684875\n",
      "Loss: 0.6774874925613403\n",
      "Loss: 0.704915463924408\n",
      "Loss: 0.684978187084198\n",
      "Loss: 0.6879230737686157\n",
      "Loss: 0.7002021670341492\n",
      "Loss: 0.6861664056777954\n",
      "Loss: 0.6773105263710022\n",
      "Loss: 0.6953616142272949\n",
      "Loss: 0.7062240242958069\n",
      "Loss: 0.6924906969070435\n",
      "Loss: 0.7046636343002319\n",
      "Loss: 0.6840645670890808\n",
      "Loss: 0.7029575109481812\n",
      "Loss: 0.7173795700073242\n",
      "Loss: 0.7171492576599121\n",
      "Loss: 0.7196885347366333\n",
      "Loss: 0.6898174285888672\n",
      "Loss: 0.6969984769821167\n",
      "Loss: 0.6842429637908936\n",
      "Loss: 0.6934326887130737\n",
      "Loss: 0.6829007267951965\n",
      "Loss: 0.6838144659996033\n",
      "Loss: 0.6557561755180359\n",
      "Loss: 0.6898713707923889\n",
      "Loss: 0.7031723856925964\n",
      "Loss: 0.7086831331253052\n",
      "Loss: 0.701090931892395\n",
      "Loss: 0.7036690711975098\n",
      "Loss: 0.6864128708839417\n",
      "Loss: 0.6890137195587158\n",
      "Loss: 0.6995823383331299\n",
      "Loss: 0.6809940338134766\n",
      "Loss: 0.7032515406608582\n",
      "Loss: 0.6708309054374695\n",
      "Loss: 0.6904327273368835\n",
      "Loss: 0.7081525921821594\n",
      "Loss: 0.7044671177864075\n",
      "Loss: 0.6964523196220398\n",
      "Loss: 0.6798526644706726\n",
      "Loss: 0.6966740489006042\n",
      "Loss: 0.6987916827201843\n",
      "Loss: 0.7096590399742126\n",
      "Loss: 0.7014049291610718\n",
      "Loss: 0.6600682735443115\n",
      "Loss: 0.704463005065918\n",
      "Loss: 0.699535071849823\n",
      "Loss: 0.6908336281776428\n",
      "Loss: 0.6846076846122742\n",
      "Loss: 0.6848876476287842\n",
      "Loss: 0.7136313319206238\n",
      "Loss: 0.6858561635017395\n",
      "Loss: 0.66374272108078\n",
      "Loss: 0.6893284320831299\n",
      "Loss: 0.7000164985656738\n",
      "Loss: 0.684807538986206\n",
      "Loss: 0.6928468346595764\n",
      "Loss: 0.6890610456466675\n",
      "Loss: 0.677124559879303\n",
      "Loss: 0.6764582395553589\n",
      "Loss: 0.7024495005607605\n",
      "Loss: 0.6909874677658081\n",
      "Loss: 0.6798858642578125\n",
      "Loss: 0.6769796013832092\n",
      "Loss: 0.7073503732681274\n",
      "Loss: 0.6651412844657898\n",
      "Loss: 0.7045301198959351\n",
      "Loss: 0.6742796301841736\n",
      "Loss: 0.6884193420410156\n",
      "Loss: 0.7011523842811584\n",
      "Loss: 0.67780601978302\n",
      "Loss: 0.7044139504432678\n",
      "Loss: 0.6735573410987854\n",
      "Loss: 0.7091854810714722\n",
      "Loss: 0.6765165328979492\n",
      "Loss: 0.6704978346824646\n",
      "Loss: 0.6677486896514893\n",
      "Loss: 0.676896333694458\n",
      "Loss: 0.6865618228912354\n",
      "Loss: 0.6920002698898315\n",
      "Loss: 0.6900947093963623\n",
      "Loss: 0.6670774817466736\n",
      "Loss: 0.6917027235031128\n",
      "Loss: 0.6635685563087463\n",
      "Loss: 0.7260226011276245\n",
      "Loss: 0.6824442148208618\n",
      "Loss: 0.6825827956199646\n",
      "Loss: 0.6819509863853455\n",
      "Loss: 0.6892198920249939\n",
      "Loss: 0.6762182712554932\n",
      "Loss: 0.6912757754325867\n",
      "Loss: 0.6771961450576782\n",
      "Loss: 0.7039521932601929\n",
      "Loss: 0.7128382325172424\n",
      "Loss: 0.6926371455192566\n",
      "Loss: 0.6767861843109131\n",
      "Loss: 0.6965326070785522\n",
      "Loss: 0.7022823691368103\n",
      "Loss: 0.7053546905517578\n",
      "Loss: 0.6495095491409302\n",
      "Loss: 0.692629873752594\n",
      "Loss: 0.6671128273010254\n",
      "Loss: 0.6883352994918823\n",
      "Loss: 0.7118877172470093\n",
      "Loss: 0.6753509640693665\n",
      "Loss: 0.6822618842124939\n",
      "Loss: 0.6886895298957825\n",
      "Loss: 0.714145839214325\n",
      "Loss: 0.6823934316635132\n",
      "Loss: 0.6874561309814453\n",
      "Loss: 0.6723664999008179\n",
      "Loss: 0.717703640460968\n",
      "Loss: 0.6956181526184082\n",
      "Loss: 0.677270233631134\n",
      "Loss: 0.680933952331543\n",
      "Loss: 0.7068424224853516\n",
      "Loss: 0.703879177570343\n",
      "Loss: 0.6951143741607666\n",
      "Loss: 0.6827627420425415\n",
      "Loss: 0.694856584072113\n",
      "Loss: 0.6632756590843201\n",
      "Loss: 0.690703809261322\n",
      "Loss: 0.7022247314453125\n",
      "Loss: 0.6631801724433899\n",
      "Loss: 0.6836750507354736\n",
      "Loss: 0.6842247247695923\n",
      "Loss: 0.6598289012908936\n",
      "Loss: 0.6899115443229675\n",
      "Loss: 0.710554301738739\n",
      "Loss: 0.6864022016525269\n",
      "Loss: 0.6946588158607483\n",
      "Loss: 0.6767086982727051\n",
      "Loss: 0.7030086517333984\n",
      "Loss: 0.6892178058624268\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "train(model,dataloader_attack,optimizer,criterion,4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'attack_model_mobilenet_cifar.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Evaluation</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"eval.p\"\n",
    "\n",
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "with open(DATA_PATH, \"rb\") as f:\n",
    "    dataset = pickle.load(f)\n",
    "# Convert all tensors to the same dtype first\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=1 , shuffle=False, num_workers=2)\n",
    "#splitting\n",
    "for batch_idx, (img, label, membership) in enumerate(dataloader):\n",
    "    img = img.to(device)\n",
    "\n",
    "\n",
    "# Define your evaluation function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.7103, -4.6378, -0.1289,  2.9930,  1.0985,  5.1932, -1.2353, -0.8289,\n",
      "         -3.7446, -4.0443]])\n",
      "tensor([[ 2.4306, -2.3788, -0.4651, -3.2092,  1.2412, -1.4609, -6.4277,  1.3404,\n",
      "          0.1524, -0.1557]])\n",
      "tensor([[-5.4066, -2.9340,  1.5373, -0.4616, -2.1943,  2.6823,  2.5824,  2.1791,\n",
      "         -3.8598, -2.3940]])\n",
      "tensor([[-1.7397,  5.6713, -4.5093, -0.4556, -1.6378, -5.0307, -0.3070,  4.7039,\n",
      "         -5.1625, -1.9458]])\n",
      "tensor([[ 0.7037, -1.1619, -0.0768,  1.4369, -0.4303, -2.4882, -2.0719, -0.5609,\n",
      "          2.1269, -2.9825]])\n",
      "tensor([[-0.7748, -1.5942, -1.1614, -0.5312, -3.9587, -2.6068,  5.3597,  2.3739,\n",
      "         -4.1059, -0.3337]])\n",
      "tensor([[ 2.3683, -1.8953, -3.5252, -4.8749,  3.6337, -1.5636, -0.2816, -2.9056,\n",
      "          3.8003, -3.8218]])\n",
      "tensor([[-3.7642, -0.1180, -4.1284, -0.2648,  0.5093,  0.9776,  2.6419,  1.4606,\n",
      "         -4.5504, -0.5863]])\n",
      "tensor([[-0.3845, -2.8281, -3.8035, -3.1389,  3.0722, -3.4887, -5.4458,  6.7233,\n",
      "         -2.1374,  2.2141]])\n",
      "tensor([[ 1.3008, -3.6743,  0.6059,  3.1263, -1.4012, -3.6087, -6.8582, -5.6540,\n",
      "          3.6711, -0.3371]])\n",
      "tensor([[-3.5264,  0.6430, -6.1034,  3.5103, -0.0688, -0.6538, -5.4034, -1.3087,\n",
      "          1.6107,  2.2025]])\n",
      "tensor([[-1.5007, -3.7168, -1.5533,  1.1067,  2.4241, -1.4960,  0.7307,  2.7716,\n",
      "         -2.8389, -3.4610]])\n",
      "tensor([[-4.3459,  3.3473, -1.9512, -3.5392,  2.1286, -2.9471,  2.8946, -5.3332,\n",
      "          4.0510, -1.3364]])\n",
      "tensor([[-5.5553, -5.9393,  3.4106,  1.3524,  4.4497,  0.9563,  1.9590,  2.0714,\n",
      "         -6.9128, -6.3207]])\n",
      "tensor([[-3.4152, -0.3133, -1.7971, -0.4869, -2.0771,  7.1992,  0.1895,  0.0651,\n",
      "         -6.7477, -0.9755]])\n",
      "tensor([[-3.0514, -0.2484, -5.1393,  2.4827, -1.6797,  1.2165,  2.3750,  0.7209,\n",
      "         -3.0254, -1.9332]])\n",
      "tensor([[-0.5561, -0.3395, -4.5626, -3.6478, -0.3236, -7.9424,  0.0627, -0.0205,\n",
      "          1.9817,  6.3195]])\n",
      "tensor([[-2.5185, -0.7720,  3.5447,  2.3738, -2.1187, -0.7491, -0.4887, -0.9095,\n",
      "         -3.8911, -1.6326]])\n",
      "tensor([[ 5.5072, -2.6571, -1.1305, -4.5913,  2.1029, -4.3081, -4.9157, -3.4452,\n",
      "          6.3341, -1.9034]])\n",
      "tensor([[-2.8115,  0.0399,  1.5265,  0.3124,  1.7815,  1.3351, -4.2766, -3.9316,\n",
      "         -0.5058, -1.1537]])\n",
      "tensor([[-4.6143, -0.0369, -1.9943,  0.8785,  0.8051,  3.3663, -0.1690, -2.5462,\n",
      "         -4.0938, -0.2865]])\n",
      "tensor([[ 0.0585, -1.3455,  1.7605, -0.7237,  2.6339,  1.1851, -3.6412, -1.9874,\n",
      "         -1.9718, -2.1795]])\n",
      "tensor([[ 2.2785, -1.4860,  0.3001, -2.1457, -3.2669, -4.0888, -3.8147, -1.1739,\n",
      "          5.5863,  0.1614]])\n",
      "tensor([[ 2.0222, -2.0753, -3.1219, -1.9029, -2.2789, -4.1532, -3.0128, -3.5889,\n",
      "          7.0833,  2.7975]])\n",
      "tensor([[-4.6017,  3.6357, -3.1844, -0.2155, -5.6099,  6.1240,  3.9052, -6.3632,\n",
      "         -4.2263,  0.4055]])\n",
      "tensor([[-5.0416,  0.8201, -2.4636, -1.6203, -5.2109,  6.0348,  1.0422, -1.2616,\n",
      "         -3.2696,  2.0410]])\n",
      "tensor([[-5.0679,  5.4201, -4.2281, -0.5395,  3.3677, -2.7791,  1.4242, -1.8278,\n",
      "         -2.3491, -0.6377]])\n",
      "tensor([[-5.2119, -0.7912, -2.9587,  5.2036,  1.6881,  0.5452,  0.0447, -3.6765,\n",
      "         -1.3314, -1.0029]])\n",
      "tensor([[-0.4177,  0.5824, -1.4473, -0.2941, -3.1608,  2.3712, -3.4387, -1.7025,\n",
      "         -1.5571,  2.5226]])\n",
      "tensor([[-3.6592,  2.9449, -2.7634,  0.8455, -0.7680, -1.8716, -1.2671, -0.2309,\n",
      "         -2.0251,  1.6084]])\n",
      "tensor([[-7.4155, -0.4997, -3.8647, -0.7092,  0.2716,  5.0431,  2.2023,  1.3258,\n",
      "         -2.3785, -3.3705]])\n",
      "tensor([[ 3.3181,  0.6812,  0.3098, -2.0315, -2.0403, -4.2489, -4.6088, -0.3851,\n",
      "         -0.0670,  0.7975]])\n",
      "tensor([[-6.5736,  0.1533, -1.0111, -0.8118, -4.7251,  3.0744,  1.6826, -0.9695,\n",
      "         -2.0694,  2.9117]])\n",
      "tensor([[-3.5087,  0.1048,  2.7685,  3.4845, -3.6175,  1.6672,  0.8545, -5.1013,\n",
      "         -1.7316, -3.9126]])\n",
      "tensor([[-1.2854, -1.4212,  2.8830,  0.3870,  1.3479,  0.5940, -4.4237, -0.4574,\n",
      "         -2.8050, -1.1950]])\n",
      "tensor([[-2.6670,  4.6219, -3.1402,  3.3624, -4.5924, -1.9862, -0.4491, -7.4489,\n",
      "          1.9432, -0.5121]])\n",
      "tensor([[ 0.9084,  2.0063, -0.8799, -0.6473,  3.3346, -3.8638, -4.3928, -1.6827,\n",
      "         -1.6146,  0.1187]])\n",
      "tensor([[ 2.4087, -0.7415, -3.1229, -3.6050, -1.1271, -1.2980, -3.5519, -5.1266,\n",
      "          7.1812, -0.3810]])\n",
      "tensor([[-8.8651,  0.6087, -2.1692,  2.3788, -6.2146,  5.0233,  6.1533, -3.0764,\n",
      "         -6.2623, -0.3795]])\n",
      "tensor([[-4.4027, -3.5971,  0.7520, -0.7654,  2.2107,  0.6350,  3.5029, -1.2865,\n",
      "         -3.2334, -0.4947]])\n",
      "tensor([[ 1.2685,  0.6282,  4.7227, -6.8277, -1.5626, -5.4722, -3.7651, -6.5431,\n",
      "          7.8500, -0.9603]])\n",
      "tensor([[ 7.2856, -4.8918,  5.3477, -4.6445, -0.2141, -4.3245, -6.1230, -6.6904,\n",
      "          8.0939, -5.2108]])\n",
      "tensor([[-1.2064, -1.8118,  3.4513,  0.3112, -1.0340,  1.0158, -1.8496, -3.8526,\n",
      "         -0.3798, -1.5768]])\n",
      "tensor([[-7.7369,  2.7814,  0.3593,  3.7057, -5.4989, -3.8668,  1.8999,  2.1242,\n",
      "         -3.2512, -0.6390]])\n",
      "tensor([[-2.8849, -1.5268, -0.4163, -1.1446, -1.7168,  0.9140, -0.1463,  0.8662,\n",
      "         -2.9921,  2.5260]])\n",
      "tensor([[ 0.2443, -0.8045,  1.4818, -2.8326, -5.1237, -0.6692, -2.4932,  5.8429,\n",
      "         -2.9962,  0.1753]])\n",
      "tensor([[-8.4136, -4.1674, -9.8940,  4.7189, -1.0669,  5.6884,  6.0768, -2.7087,\n",
      "         -2.0224, -3.0829]])\n",
      "tensor([[-0.4962,  1.0713,  1.5978,  0.1818, -4.8585,  1.0259, -6.9077,  8.3405,\n",
      "         -6.4135, -2.2904]])\n",
      "tensor([[ 2.8662, -2.5815,  0.7974,  1.0047, -5.0596, -2.7170, -6.9807,  4.0438,\n",
      "         -3.0625,  2.9109]])\n",
      "tensor([[-1.0666,  2.3227,  2.9308, -0.2557, -2.8596, -5.0146, -8.5698,  3.0200,\n",
      "         -4.9226,  2.3816]])\n",
      "tensor([[-2.7837,  4.3228, -4.9673, -0.5027,  0.1524, -1.1525,  0.4441,  2.5701,\n",
      "         -3.4531, -1.1988]])\n",
      "tensor([[ 1.5432,  0.9883, -2.6391, -3.1407, -0.2173, -2.3325, -3.4679, -4.1537,\n",
      "          6.1296, -0.3754]])\n",
      "tensor([[ 1.6354, -3.3285,  2.7480, -2.0606,  0.0903, -4.9659, -1.5717, -4.0954,\n",
      "          3.7506, -0.0267]])\n",
      "tensor([[-6.5835, -2.2807,  3.9878,  0.8697,  2.2565, -0.5487,  0.2474,  1.7856,\n",
      "         -3.9071, -3.9033]])\n",
      "tensor([[-2.2164,  1.3308,  0.4761, -0.6457,  0.1186, -0.3199, -2.5515, -0.4067,\n",
      "         -3.0673,  0.6726]])\n",
      "tensor([[-2.2191, -3.6387,  2.6500,  2.0565,  3.4647, -1.1108,  0.2751, -2.9402,\n",
      "         -1.4291, -2.9917]])\n",
      "tensor([[-7.3568, -4.6197,  1.9200,  2.8123,  2.6862,  3.2502,  2.8945,  1.1257,\n",
      "         -8.9699, -3.9299]])\n",
      "tensor([[-6.1681, -4.1889,  3.9129, -0.7117,  6.9233, -2.3909,  1.0689,  0.1042,\n",
      "         -3.0736, -4.8596]])\n",
      "tensor([[-1.0533, -0.8071, -0.5373, -1.0724, -4.1767,  0.0224, -2.3588,  5.7221,\n",
      "         -2.5508, -0.8606]])\n",
      "tensor([[ 4.2435, -0.7225,  0.1318, -1.9258, -3.4674, -4.1582, -2.5836,  0.8714,\n",
      "          0.1456,  1.0117]])\n",
      "tensor([[-4.4204, -2.4663,  0.5283,  2.2918,  2.3131, -0.0120,  4.4253, -2.0929,\n",
      "         -4.3209, -2.7284]])\n",
      "tensor([[-2.8302,  1.3194,  0.5751, -1.0224,  2.5395,  2.2265, -5.2242,  1.4522,\n",
      "         -5.5948, -1.9343]])\n",
      "tensor([[-3.8845, -1.1882, -2.5616, -0.5207,  3.0432, -0.1590,  1.0368,  1.9207,\n",
      "         -0.3166, -3.8016]])\n",
      "tensor([[-0.9844, -1.8852, -3.6626, -3.6601, -3.0965, -0.2287, -2.5821, -8.0776,\n",
      "          5.8419,  4.9067]])\n",
      "tensor([[-3.0370,  0.6089,  0.1023, -1.5698, -5.8039, -2.9290, -0.0448,  6.5637,\n",
      "         -0.5528, -1.7482]])\n",
      "tensor([[-6.1174,  1.8981,  1.1259, -3.0760, -0.6234, -0.6085,  0.0162, -0.4742,\n",
      "          0.0289,  0.8875]])\n",
      "tensor([[-6.1687, -5.5727,  3.6368, -0.8921,  4.1723, -3.0062,  4.7822,  2.8783,\n",
      "         -3.4462, -5.6096]])\n",
      "tensor([[-3.2961, -3.8206,  2.1931, -2.4539,  2.8291,  1.5262, -0.5600,  3.6026,\n",
      "         -5.1465, -2.4149]])\n",
      "tensor([[-0.5372,  9.9379, -5.2308, -8.9931,  0.0571, -0.5417, -3.2144, -1.9032,\n",
      "         -3.0124,  1.9128]])\n",
      "tensor([[-1.6996, -0.1781,  3.5473,  3.8696,  2.8779, -2.7213,  0.8392, -5.8523,\n",
      "         -2.1479, -5.0863]])\n",
      "tensor([[-0.8756, -0.5756, -2.5764, -1.6829, -0.5009, -1.5212, -1.6004, -1.2784,\n",
      "          4.4934, -1.8032]])\n",
      "tensor([[-0.6603,  3.2018, -3.8410,  2.6730, -2.0399,  0.2054, -5.5558, -5.1501,\n",
      "         -1.1591,  3.3480]])\n",
      "tensor([[-0.2449,  3.9677, -0.6756, -2.6442, -3.9385, -0.1724, -7.1044,  2.1831,\n",
      "         -1.6053,  2.1595]])\n",
      "tensor([[ 5.4261,  1.5567, -3.1616, -0.8683, -3.5124, -6.6655, -3.2705,  0.6098,\n",
      "         -2.0177,  3.4763]])\n",
      "tensor([[ 0.8268, -4.4154, -0.7947,  2.9255, -0.0833, -0.0667, -3.7860, -2.0607,\n",
      "          2.4589, -1.7089]])\n",
      "tensor([[-5.1895,  1.2089, -1.0758,  2.9401, -2.5200,  2.8038, -3.7837,  3.1049,\n",
      "         -2.7069, -2.2087]])\n",
      "tensor([[-4.3399,  0.3794,  1.0703,  2.5413, -7.2091, -4.1913, -3.7591, -3.8539,\n",
      "          6.0814,  1.6957]])\n",
      "tensor([[-1.1176, -1.9395,  1.1806,  2.6195,  0.9279, -1.8129, -2.7952, -2.6483,\n",
      "          1.2202, -2.6013]])\n",
      "tensor([[ 1.6263, -3.7678,  4.3061,  0.4309,  1.8600, -2.9534, -1.4194, -2.2055,\n",
      "         -1.0048, -3.0580]])\n",
      "tensor([[ 1.7269, -1.7513,  3.1094,  1.9052, -4.7120, -0.9444,  2.0295, -3.0470,\n",
      "         -0.6771, -3.5845]])\n",
      "tensor([[-4.1067, -1.9155,  1.4956, -0.1877,  0.5167,  2.7333, -1.1741, -0.1169,\n",
      "         -3.7813, -0.5519]])\n",
      "tensor([[ 3.4752, -3.9976,  5.4477, -4.2478,  2.0548, -1.9690, -6.5342, -4.7376,\n",
      "          3.5898, -2.1122]])\n",
      "tensor([[-0.2111, -0.1797, -0.3331, -2.2341, -2.4359, -1.0212, -4.9844, -1.1819,\n",
      "          0.3437,  3.3816]])\n",
      "tensor([[-2.4018, -6.4941,  7.9442,  0.7439,  2.6477, -4.0184,  5.1498, -3.0746,\n",
      "         -2.0803, -7.5248]])\n",
      "tensor([[-2.5804,  1.2820, -2.0084, -2.6303, -5.0668, -0.7588, -0.8587,  0.7421,\n",
      "          0.1506,  3.9719]])\n",
      "tensor([[ 4.5647, -0.7494,  1.2774, -2.0714, -1.6906, -3.0054, -4.0237, -1.1773,\n",
      "         -0.2781, -0.1432]])\n",
      "tensor([[-7.2167,  0.1902, -2.8285,  3.2350, -4.9962,  3.8254, -0.4554,  0.7462,\n",
      "         -2.2944,  1.0802]])\n",
      "tensor([[-3.2121, -5.3868,  1.1551,  3.0266,  5.2683, -0.7965,  1.5567,  0.3537,\n",
      "         -3.4789, -5.2346]])\n",
      "tensor([[ 3.1313,  1.2072, -1.3687, -0.9687, -2.5152, -6.8933, -6.0659, -1.2276,\n",
      "          4.4724, -0.4052]])\n",
      "tensor([[ 7.1904, -3.4010, -0.3481, -5.1522,  1.3901, -1.9176, -5.8612, -5.9943,\n",
      "          6.3947, -2.7968]])\n",
      "tensor([[-3.6391, -1.6110, -1.3419,  5.5095, -1.4774,  1.1802,  4.8709, -3.3293,\n",
      "         -4.4000, -2.4643]])\n",
      "tensor([[-7.6668, -0.8801,  0.2344,  1.9658,  1.2054,  2.1312,  5.6589, -0.5790,\n",
      "         -7.4502, -5.0751]])\n",
      "tensor([[ 1.7431, -3.1933,  1.3431, -4.5494, -1.3871, -0.5045,  0.4009,  1.1591,\n",
      "         -0.8200, -0.2533]])\n",
      "tensor([[ 4.2140,  3.2355, -6.3082, -2.0788, -2.4692, -4.2051, -0.9168, -1.4129,\n",
      "          0.6517,  0.2663]])\n",
      "tensor([[-5.8038, -2.4022, -0.1224,  3.8990, -0.6018,  5.2929,  2.1309, -3.4374,\n",
      "         -3.2823, -3.8776]])\n",
      "tensor([[-6.6532, -3.2903, -0.2097,  5.9738, -2.5427, -0.2223, -0.6293,  1.9492,\n",
      "         -5.6908,  1.3940]])\n",
      "tensor([[ 2.9955, -5.8452,  4.3521,  0.4721, -2.4953, -2.2175, -2.7649, -0.8580,\n",
      "          1.5773, -2.7319]])\n",
      "tensor([[ 1.9214, -2.7222,  0.1793, -4.3400, -0.2669, -2.3684, -4.3906, -2.7802,\n",
      "          5.0362,  0.8169]])\n",
      "tensor([[-4.8050, -6.0485,  5.5486,  0.6879,  7.1468, -1.2553,  3.0082, -2.5891,\n",
      "         -4.8412, -5.6901]])\n",
      "tensor([[-2.1346, -1.5985, -1.9544, -0.7863,  4.3531,  1.1818, -0.3842,  0.4127,\n",
      "         -4.2998, -1.6040]])\n",
      "tensor([[-2.7494, -4.7151,  6.4326,  2.1749,  2.1860, -2.2291,  3.0602, -1.4503,\n",
      "         -5.1317, -7.9404]])\n",
      "tensor([[-4.2972, -0.1017,  1.5857,  0.6026, -2.3485,  0.2284,  1.8122,  2.4667,\n",
      "         -3.0797, -2.4821]])\n",
      "tensor([[ 2.4698,  4.9767,  0.4218, -2.4994, -7.1784, -5.0938, -4.5375,  0.1469,\n",
      "         -8.3017,  6.8460]])\n",
      "tensor([[ 1.7399,  0.0908, -1.2161, -3.5079,  0.0405, -6.2033, -4.8492,  0.5441,\n",
      "          5.4286, -0.6304]])\n",
      "tensor([[-1.9106,  0.6109,  5.4217, -2.6302, -0.9576, -2.7846, -0.5443, -2.5612,\n",
      "         -1.4868,  0.2195]])\n",
      "tensor([[-5.1800, -1.1530, -1.4542, -1.1893,  0.6472,  1.9044,  2.9612,  4.9775,\n",
      "         -4.8330, -5.0178]])\n",
      "tensor([[-2.2731, -0.3762,  0.8805, -0.9834, -3.7251, -1.2253,  3.4718, -0.1004,\n",
      "         -4.4694,  1.8251]])\n",
      "tensor([[-2.9050,  6.9611, -2.1665, -4.5816,  0.0643, -4.8269, -2.0515, -3.9042,\n",
      "          4.9703, -0.6709]])\n",
      "tensor([[-0.6429, -5.4486,  6.7622, -2.1869,  3.0939, -3.5092, -0.0449, -1.4925,\n",
      "         -0.2368, -5.2622]])\n",
      "tensor([[-4.8704, -3.6060, -0.5159, -0.6906,  4.0147,  2.4295,  2.1667,  0.0071,\n",
      "         -1.8782, -3.5862]])\n",
      "tensor([[-5.0241, -1.4654,  0.6265,  1.9654, -3.9794, -1.6801,  2.1890,  0.9784,\n",
      "         -0.9857, -1.0325]])\n",
      "tensor([[-2.7951, -1.0938, -4.3839, -0.5683, -0.6905,  3.1189,  1.6605, -1.7349,\n",
      "         -2.1856,  1.5483]])\n",
      "tensor([[ 2.7435, -3.1373,  4.0206, -2.3817, -0.5593, -0.3677, -8.9391, -2.3264,\n",
      "          2.2801, -2.0043]])\n",
      "tensor([[-3.8535,  0.3274, -3.5042, -1.2232, -1.5452,  0.7387, -1.9633,  3.6633,\n",
      "         -5.5255,  4.4741]])\n",
      "tensor([[  4.2754,  -6.9242,  14.8359,   1.1241,  -1.1604,   2.6565,  -5.7410,\n",
      "          -9.9749,  -5.0751, -11.2794]])\n",
      "tensor([[ 9.5868, -2.1972, -0.0207, -5.2433, -1.2381, -5.2354, -7.2206, -7.0112,\n",
      "          7.4842, -1.2165]])\n",
      "tensor([[ 0.2219, -4.2083,  0.3328,  1.9818,  2.0092,  4.4890,  1.3833, -5.1555,\n",
      "         -3.0099, -6.7949]])\n",
      "tensor([[ 2.3715, -3.4630, -2.2560, -0.8733,  1.0746, -0.4485, -3.6417, -3.9565,\n",
      "          4.8584, -2.0734]])\n",
      "tensor([[-4.5651, -2.9189,  5.7030, -1.2164,  3.9055,  0.3419,  2.7454, -0.1725,\n",
      "         -6.4924, -6.5635]])\n",
      "tensor([[-0.7292,  9.3258, -6.7163,  0.7929, -6.4488, -1.7169, -4.0339, -1.5989,\n",
      "         -1.0298,  2.0642]])\n",
      "tensor([[-4.4841,  1.8702, -3.8193, -2.2002, -6.6000,  4.4600,  1.4659,  0.7483,\n",
      "         -5.5188,  4.6406]])\n",
      "tensor([[-2.5636, -4.8484,  2.4837, -1.9237, -6.3399, -1.1863, -0.2734,  1.8547,\n",
      "         -2.6593,  5.4767]])\n",
      "tensor([[-3.7494,  2.5988, -0.7149,  0.1593, -0.3162, -0.8461, -2.0751, -0.9221,\n",
      "         -1.8763,  0.4295]])\n",
      "tensor([[ 0.2546, -3.5372,  1.8953,  2.3430,  2.2020, -0.8309, -3.5431, -4.5101,\n",
      "          2.1635, -3.1909]])\n",
      "tensor([[-2.2850, -1.6113, -3.9955,  0.5470,  3.5552,  1.8890,  0.0870, -3.9989,\n",
      "          2.1817, -4.1142]])\n",
      "tensor([[ 1.4654, -1.9392, -2.5440,  2.7354, -1.3915, -4.6762, -2.9370, -1.9531,\n",
      "          3.7562,  0.6116]])\n",
      "tensor([[-2.6919, -4.3092,  3.6175,  0.7411, -1.0807,  1.8104, -1.1926, -0.9969,\n",
      "         -4.5383,  0.4910]])\n",
      "tensor([[-6.5204, -3.9312,  2.9219, -1.3193,  5.4675,  0.2560,  4.9233, -2.0731,\n",
      "         -1.0114, -7.5463]])\n",
      "tensor([[ 1.0277, -4.7321,  9.2005,  0.6413, -2.6573, -1.1560,  0.7040, -3.0892,\n",
      "         -2.6093, -8.4051]])\n",
      "tensor([[-1.7416, -6.0282,  3.4594,  2.6131,  2.0049, -4.5861, -0.7815, -0.2197,\n",
      "         -0.6245, -2.0026]])\n",
      "tensor([[ 4.5297, -1.3562, -1.1318, -3.9301, -2.3701, -5.0712, -5.3893, -1.1269,\n",
      "          5.6595,  0.8965]])\n",
      "tensor([[-0.7319, -2.8601, -2.0831,  4.6924,  1.2787, -3.2774, -0.4706, -1.3943,\n",
      "          0.9859, -2.6696]])\n",
      "tensor([[-5.5659,  0.9024, -3.4967,  4.7946, -4.2622,  0.6118,  1.5203,  0.0402,\n",
      "         -3.2829, -0.6874]])\n",
      "tensor([[-4.2766, -0.8169,  1.6625,  3.1013,  0.0423, -1.4095,  2.2463,  0.9412,\n",
      "         -3.9010, -4.8592]])\n",
      "tensor([[ 1.9995, -0.3975, -2.0082, -0.0499,  0.1114, -2.6693, -5.6753, -5.8832,\n",
      "          6.4557, -1.2448]])\n",
      "tensor([[-5.0732,  3.4116, -2.4404,  0.4761, -2.9833,  4.5339, -1.4399, -0.2153,\n",
      "         -5.3179,  0.0160]])\n",
      "tensor([[-4.1441, -2.4305, -0.4394,  2.7105, -3.7388,  5.1922, -1.9047,  1.7230,\n",
      "         -2.6331, -4.5991]])\n",
      "tensor([[-1.3831, -0.8104,  3.4083,  0.4113, -3.2828,  0.1255, -1.5044,  2.8371,\n",
      "         -4.9645, -2.6408]])\n",
      "tensor([[-4.7252, -3.3056, -1.2730,  0.3642,  5.6015,  2.3878,  1.6385, -3.8839,\n",
      "         -3.6748, -0.6188]])\n",
      "tensor([[ 3.2245, -4.9496,  5.2395, -6.1480,  2.2033, -4.1478, -4.1629, -8.0603,\n",
      "          9.9909, -5.0149]])\n",
      "tensor([[-3.6748, -2.2065,  2.7580,  2.6503, -1.0323,  4.5013, -1.6363,  0.7851,\n",
      "         -5.8529, -5.4074]])\n",
      "tensor([[-6.7719,  1.4611, -2.4086, -0.2904, -0.4593, -1.8506,  0.9609,  3.7080,\n",
      "         -0.0998, -2.8717]])\n",
      "tensor([[-0.9260, -0.1822, -3.9858, -4.5336,  0.0322, -2.3174, -3.4620, -3.1146,\n",
      "          6.8194,  2.0103]])\n",
      "tensor([[ 1.8923,  3.5592,  0.5696, -3.0500, -4.3445, -5.5523, -6.6497, -1.7637,\n",
      "          5.1949, -0.5162]])\n",
      "tensor([[-4.3450, -0.7528,  3.6018, -3.2500,  0.8724,  4.2825,  1.9870, -3.8836,\n",
      "         -4.8923, -2.4544]])\n",
      "tensor([[ 7.3384, -4.6656, -0.2516, -0.5902, -3.2710, -0.1271, -4.7209, -5.9312,\n",
      "          3.9038, -1.9713]])\n",
      "tensor([[ 0.8224, -2.4545, -0.8673,  0.3209,  3.2156, -5.1854, -1.6002,  5.6024,\n",
      "         -5.5672, -2.8974]])\n",
      "tensor([[ 0.5948,  5.1838, -6.3892, -0.6508, -1.4953, -1.3171, -1.8541,  0.3365,\n",
      "         -2.8161,  0.7425]])\n",
      "tensor([[-5.4692,  1.3346, -2.8803,  3.5683,  0.3240,  1.2377, -1.1431,  1.9053,\n",
      "         -3.8348, -3.4487]])\n",
      "tensor([[-5.9894, -3.2466,  0.6199,  3.7258, -0.0081,  2.0672,  2.0968, -1.5951,\n",
      "         -3.3329, -2.6651]])\n",
      "tensor([[-0.9484, -5.2295, -3.0075,  0.9998,  2.2231,  3.2655, -0.2537,  0.5341,\n",
      "         -2.5576, -2.0090]])\n",
      "tensor([[ 1.6453,  6.4922, -0.4903, -5.1971, -3.2638,  0.2155, -2.1135, -3.5093,\n",
      "         -4.2129,  0.8963]])\n",
      "tensor([[-1.9812, -0.7380, -2.1681,  3.9042, -1.5919,  3.0364, -2.1794, -2.5454,\n",
      "         -2.4975, -0.8337]])\n",
      "tensor([[-2.5495,  5.9050, -6.9867,  5.3053, -3.3599, -7.2545,  0.4683,  1.3358,\n",
      "         -6.7631,  2.0178]])\n",
      "tensor([[-4.4242,  4.6513, -5.4500, -5.3423, -0.8283, -5.6030, -1.5097, -3.4410,\n",
      "          8.0153,  1.6673]])\n",
      "tensor([[ 4.9423, -4.3126,  5.6818, -2.7653, -1.3342, -1.3212, -3.9619, -1.1276,\n",
      "         -0.7405, -5.6311]])\n",
      "tensor([[-1.7339, -2.5594, -0.8472,  0.8301, -6.1923, -4.8174,  1.4829,  4.8056,\n",
      "         -6.5369,  3.8868]])\n",
      "tensor([[ 2.2912, -2.0237, -2.1059, -1.6411,  0.9352, -1.7944, -3.8377,  0.5742,\n",
      "          2.3696, -0.4510]])\n",
      "tensor([[-3.9092,  0.1856,  0.2258,  0.1836,  0.3826,  1.9876, -5.2208, -0.6725,\n",
      "         -3.9862,  1.6943]])\n",
      "tensor([[ 9.9411, -7.3039,  1.7110, -2.2761, -4.2608, -2.9097, -3.5984, -4.6669,\n",
      "          4.1834, -1.9283]])\n",
      "tensor([[-4.6820,  2.9403, -2.7317, -1.3803, -3.0537, -4.7807,  2.3675,  2.1278,\n",
      "         -2.6745,  3.9124]])\n",
      "tensor([[-3.5807, -1.9306,  0.6098, -0.5688, -6.3430, -1.5672,  7.2921,  1.6342,\n",
      "         -1.3978, -3.3859]])\n",
      "tensor([[ 2.2526, -2.6468, -3.1013,  4.1563, -3.3821, -1.3115, -4.0530, -3.5944,\n",
      "          3.8771,  0.0739]])\n",
      "tensor([[ 2.6629, -2.8601,  1.4678,  0.7364,  0.5045, -4.9958, -5.0173,  1.8817,\n",
      "         -0.7480, -0.9305]])\n",
      "tensor([[-4.2645, -3.9189, -1.0506,  9.2508, -0.3079,  2.7261,  1.7459, -0.5291,\n",
      "         -6.3513, -9.1281]])\n",
      "tensor([[-2.3773, -3.1619,  2.0302,  1.9386, -3.2523, -0.8911,  1.5922,  1.5111,\n",
      "         -1.7783, -2.0878]])\n",
      "tensor([[-2.7286, -2.7401,  2.1913,  3.3256,  1.0113,  3.1823, -0.7788, -1.7628,\n",
      "         -4.3385, -3.5078]])\n",
      "tensor([[ 3.1557, -2.5029,  1.5202,  1.2106, -0.7689, -1.8498, -5.6575,  3.3795,\n",
      "         -1.7971, -3.7026]])\n",
      "tensor([[-3.1402, -0.0419, -0.5806,  3.9001, -1.5641,  0.3377, -1.1564, -0.6277,\n",
      "         -2.8676, -0.6552]])\n",
      "tensor([[-2.4115, -1.1135,  0.2263, -1.8124,  0.7869, -1.8514, -0.9091,  4.9861,\n",
      "         -1.9644, -1.3592]])\n",
      "tensor([[-1.5428, -5.6903,  3.7522,  1.8986,  3.4056, -1.6856, -0.5107, -2.6374,\n",
      "          0.5808, -4.1288]])\n",
      "tensor([[-5.2343,  1.0314,  0.2830, -0.0559,  0.9743, -2.2142,  4.6089, -1.9929,\n",
      "         -3.6080, -1.2578]])\n",
      "tensor([[ 5.6664, -0.3582, -1.9799, -1.6309,  0.0569, -2.6231, -3.8537, -1.7135,\n",
      "          0.8199, -1.9096]])\n",
      "tensor([[ 0.7975,  0.3647, -0.7719, -1.0104, -3.2810, -1.5181, -4.4308,  5.4392,\n",
      "         -2.0491, -0.2996]])\n",
      "tensor([[-1.1823, -1.5891, -2.7583,  0.3941, -2.2870, -2.6013, -1.0214, -2.4037,\n",
      "          4.4012,  1.8235]])\n",
      "tensor([[-0.9278, -0.7788, -2.8477,  0.5072, -1.8594, -0.0270, -3.5360,  4.2568,\n",
      "         -3.4642,  2.0541]])\n",
      "tensor([[-5.9021,  0.2493, -5.0074, -0.3805,  0.9071,  0.3425,  0.0572,  2.6019,\n",
      "          1.1171, -0.6396]])\n",
      "tensor([[-0.2178, -5.4198,  7.2094,  0.1058,  2.1736, -5.0502,  1.4205, -5.1390,\n",
      "         -0.0675, -4.8996]])\n",
      "tensor([[ 0.8966, -4.8701,  3.5285, -2.6516,  0.0209,  2.7346, -0.3693, -3.0549,\n",
      "         -2.5126, -0.8555]])\n",
      "tensor([[-5.7260, -2.0714,  1.3953, -0.0467, -1.7385, -2.2020,  3.8734,  2.6657,\n",
      "         -3.3596, -0.8794]])\n",
      "tensor([[-5.8329,  1.3720, -2.1645, -1.2717, -1.0052,  1.1970, -1.2497,  3.4399,\n",
      "         -3.4639,  1.3502]])\n",
      "tensor([[-4.6810, -5.6634, -4.8061,  5.2831,  0.7866,  1.2818,  3.5252,  1.0917,\n",
      "         -3.8363, -1.3046]])\n",
      "tensor([[-6.6904, -1.8716, -1.7703,  5.4642, -6.2546, -0.1306, -4.8954,  1.2304,\n",
      "         -1.0027,  5.4376]])\n",
      "tensor([[-4.5766, -6.3076, -3.0504,  0.2326,  5.0900,  2.6760,  7.6598, -2.3990,\n",
      "         -3.3877, -4.9330]])\n",
      "tensor([[-2.4689,  1.6047, -2.8169,  0.8020, -4.2613,  1.8202, -2.6218, -0.0857,\n",
      "         -1.7992,  3.2407]])\n",
      "tensor([[ 1.4014, -3.6022, -1.4060,  1.1176,  1.8400,  1.5358, -2.5726, -5.1627,\n",
      "          2.0484, -1.9598]])\n",
      "tensor([[-4.3252, -7.9455,  4.5315, -2.7809,  1.6151,  2.1027,  9.9193, -1.8522,\n",
      "         -6.4268, -6.4277]])\n",
      "tensor([[-6.8342, -2.4103, -1.8876,  3.3485,  2.4732,  2.9334,  0.5283,  0.1768,\n",
      "         -3.1603, -3.7007]])\n",
      "tensor([[-4.5683,  2.1510, -4.1828,  0.0594, -1.3938,  0.2713, -0.0102,  1.0274,\n",
      "         -1.3555,  1.0754]])\n",
      "tensor([[-3.6191, -3.1807,  0.5151, -0.7061,  1.2887,  1.3493,  5.6238, -2.2842,\n",
      "         -2.7457, -2.8954]])\n",
      "tensor([[-4.1739, -4.5703, -5.0372,  0.5452,  0.7386,  3.8641,  3.5788, -1.4076,\n",
      "         -3.0258,  0.6247]])\n",
      "tensor([[-1.1902, -3.8441, -0.4028, -1.7123,  4.8884, -2.9954,  2.3254, -2.8702,\n",
      "          0.5391, -0.9917]])\n",
      "tensor([[ 2.4345,  2.6224, -4.1662, -2.1265,  0.3097, -4.7526, -1.2716, -3.8195,\n",
      "          1.7160,  0.7403]])\n",
      "tensor([[-1.5665,  1.0604,  2.6869,  0.8433,  2.2912, -1.0578, -2.9382, -2.9721,\n",
      "         -2.1187, -1.8515]])\n",
      "tensor([[-1.9915,  2.1956, -0.7575,  4.2166, -3.1976, -3.6318,  2.8434, -1.8017,\n",
      "         -2.1952, -3.7036]])\n",
      "tensor([[ 0.8992, -4.8642,  5.8054, -2.2202,  2.8136, -1.1719, -4.1721, -4.0835,\n",
      "          2.3061, -4.3467]])\n",
      "tensor([[ 1.7201, -5.6919, -3.5936, -5.3107, -1.5787,  1.5659, -1.8670,  5.7203,\n",
      "         -3.3158,  3.0369]])\n",
      "tensor([[-4.2759, -0.7312, -4.5554, -0.2526,  0.5957, -1.7573,  0.2560,  5.5171,\n",
      "         -2.7156,  0.3397]])\n",
      "tensor([[ 4.7860,  3.3924, -2.6521, -2.7093, -4.4764, -0.2463, -5.5050, -1.1432,\n",
      "          1.6170, -3.7762]])\n",
      "tensor([[-0.6099, -2.0730, -1.4084,  0.5957,  0.0137,  0.1686, -2.8955, -1.3242,\n",
      "          1.6666, -0.1274]])\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "resnet_shadow.eval()\n",
    "with torch.no_grad():\n",
    "    for images,_, b in dataloader: #need only one\n",
    "            # Move images and labels to the appropriate device\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "        logits = resnet_shadow(images)\n",
    "        print(logits)\n",
    "        \n",
    "        #take the 3 biggest logist\n",
    "        \n",
    "        top_values = torch.topk(logits, k=3).values\n",
    "        \n",
    "\n",
    "        dataset.append([top_values,b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MobileNetV2(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (14): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (15): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (16): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (17): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (18): Conv2dNormActivation(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=1280, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_target_model =  models.mobilenet_v2(pretrained=False)\n",
    "num_classes = 10  # Change this to the number of classes in your dataset\n",
    "num_features = resnet_target_model.classifier[1].in_features\n",
    "resnet_target_model.classifier = nn.Sequential(\n",
    "    #nn.Dropout(p=0.),  # Optional dropout layer for regularization\n",
    "    nn.Linear(num_features, num_classes)\n",
    ")\n",
    "check = torch.load(\"mobilenetv2_cifar10.pth\", map_location=torch.device('cpu'))\n",
    "check[\"net\"][\"classifier.0.weight\"] =check[\"net\"].pop(\"classifier.1.weight\")\n",
    "check[\"net\"][\"classifier.0.bias\"] =check[\"net\"].pop(\"classifier.1.bias\")\n",
    "\n",
    "resnet_target_model.load_state_dict(check[\"net\"])\n",
    "resnet_target_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get posteriors from target\n",
    "dataset_eval = []\n",
    "with torch.no_grad():\n",
    "    for images,_, member in dataloader: #need only one\n",
    "            # Move images and labels to the appropriate device\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "        logits = resnet_target_model(images)\n",
    "        \n",
    "        #take the 3 biggest logist\n",
    "        \n",
    "        top_values = torch.topk(logits, k=3).values #order poseri\n",
    "        sorted_tensor, indices = torch.sort(top_values, dim=1,descending=True)\n",
    "        dataset_eval.append([sorted_tensor, member.item()])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Convert all tensors to the same dtype first\\ntensors = [data[0].float() for data in dataset_eval]  # Ensure all tensors are Float type\\nall_data = torch.cat(tensors, dim=0)  # Concatenate all tensors\\n\\n# Calculate mean and std\\nmean = all_data.mean(dim=0)\\nstd = all_data.std(dim=0)\\n\\n# Standardize data in the list\\ndataset = [( (data[0] - mean) / std, data[1] ) for data in dataset_eval]'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset_eval\n",
    "\"\"\"# Convert all tensors to the same dtype first\n",
    "tensors = [data[0].float() for data in dataset_eval]  # Ensure all tensors are Float type\n",
    "all_data = torch.cat(tensors, dim=0)  # Concatenate all tensors\n",
    "\n",
    "# Calculate mean and std\n",
    "mean = all_data.mean(dim=0)\n",
    "std = all_data.std(dim=0)\n",
    "\n",
    "# Standardize data in the list\n",
    "dataset = [( (data[0] - mean) / std, data[1] ) for data in dataset_eval]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_eval = torch.utils.data.DataLoader(\n",
    "    \n",
    "    dataset, batch_size=1 , shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7559/3541459405.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([1.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[0.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[0.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[0.]]) tensor([0.])\n",
      "tensor([[0.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[0.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[0.]]) tensor([0.])\n",
      "tensor([[0.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[0.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[0.]]) tensor([0.])\n",
      "tensor([[0.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[0.]]) tensor([0.])\n",
      "tensor([[0.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[0.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[0.]]) tensor([0.])\n",
      "tensor([[0.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[0.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[0.]]) tensor([0.])\n",
      "tensor([[0.]]) tensor([0.])\n",
      "tensor([[0.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "tensor([[1.]]) tensor([0.])\n",
      "Accuracy: 0.59\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for data in dataloader:\n",
    "            inputs = data[0]\n",
    "            \n",
    "            labels = data[1].float()\n",
    "            labels = torch.tensor(labels).float()\n",
    "            #labels = torch.tensor(labels[0])\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs.squeeze(dim=1))\n",
    "            predicted = torch.round(outputs)  # Round the outputs to 0 or 1\n",
    "            #predicted = abs(predicted -1)\n",
    "            print(predicted,labels)\n",
    "\n",
    "            total += labels.size(0)  # Increment the total count by batch size\n",
    "            correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Assuming you have a DataLoader called 'test_loader' for evaluation\n",
    "# Create an instance of your model\n",
    "\n",
    "# Assuming 'test_loader' is your DataLoader for evaluation\n",
    "accuracy = evaluate(model, dataloader_eval)#\n",
    "\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
